{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flc-model\n",
    "\n",
    "\n",
    "* fractional logistic regression -> predict scores (R2, goodness-of-fit, metric weights)\n",
    "* logistic regression -> isolate high-quality units (R2, goodness-of-fit,)\n",
    "\n",
    "* simplest model: logistic regression on quality metrics (reference to literature)\n",
    "\n",
    "* we can increase the number of units by adding all non-good units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "2024-10-04 10:24:02,754 - root - utils.py - get_config - INFO - Reading experiment config.\n",
      "2024-10-04 10:24:03,037 - root - utils.py - get_config - INFO - Reading experiment config. - done\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from statsmodels.iolib.smpickle import load_pickle\n",
    "\n",
    "# set project path\n",
    "proj_path = \"/gpfs/bbp.cscs.ch/project/proj85/home/laquitai/spikebias/\"\n",
    "os.chdir(proj_path)\n",
    "\n",
    "from src.nodes.utils import get_config\n",
    "from src.nodes import utils \n",
    "from src.nodes.models.Flc import dataloader as flc_dataloader\n",
    "from src.nodes.models.Flc.models import FlcModel # FLC model\n",
    "from src.nodes.models.CebraSpike.models import CebraSpike # FLC model\n",
    "from src.nodes.models.CebraSpike import plotutils\n",
    "\n",
    "# npx spont. biophy.\n",
    "cfg_e, _ = get_config(\"silico_neuropixels\", \"stimulus\").values()\n",
    "KS4_e_10m = cfg_e[\"sorting\"][\"sorters\"][\"kilosort4\"][\"10m\"][\n",
    "    \"output\"\n",
    "]  # sorting with KS4\n",
    "GT_e_10m = cfg_e[\"sorting\"][\"simulation\"][\"ground_truth\"][\"10m\"][\"output\"] # KS4 sorting\n",
    "STUDY_e = cfg_e[\"postprocessing\"][\"waveform\"][\"sorted\"][\"study\"][\"kilosort4\"][\n",
    "    \"10m\"\n",
    "]  # WaveformExtractor\n",
    "STUDY_e_su = '/gpfs/bbp.cscs.ch/project/proj85/scratch/laquitai/preprint_2024/0_silico/4_spikesorting_stimulus_test_neuropixels_8-1-24__8slc_80f_360r_50t_200ms_1_smallest_fiber_gids/0fcb7709-b1e9-4d84-b056-5801f20d55af/postpro/realism/spike/sorted/study_ks4_10m_single_units'\n",
    "\n",
    "# PATHS\n",
    "# pre-computed sorted unit quality\n",
    "quality_path = \"/gpfs/bbp.cscs.ch/project/proj85/scratch/laquitai/preprint_2024/analysis/sorting_quality/sorting_quality.csv\"\n",
    "\n",
    "# model save path\n",
    "model_path = \"/gpfs/bbp.cscs.ch/project/proj85/scratch/laquitai/preprint_2024/analysis/sorting_quality/models/evoked/flc/model_on_full_10m_data.pickle\"\n",
    "\n",
    "# axes\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"font.size\"] = 6  # 5-7 with Nature neuroscience as reference\n",
    "plt.rcParams[\"lines.linewidth\"] = 0.5 # typically between 0.5 and 1\n",
    "plt.rcParams[\"axes.linewidth\"] = 0.5 #1\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"xtick.major.width\"] = 0.5 #0.8 #* 1.3\n",
    "plt.rcParams[\"xtick.minor.width\"] = 0.5 #0.8 #* 1.3\n",
    "plt.rcParams[\"ytick.major.width\"] = 0.5 #0.8 #* 1.3\n",
    "plt.rcParams[\"ytick.minor.width\"] = 0.5 #0.8 #* 1.3\n",
    "plt.rcParams[\"xtick.major.size\"] = 3.5 * 1.1\n",
    "plt.rcParams[\"xtick.minor.size\"] = 2 * 1.1\n",
    "plt.rcParams[\"ytick.major.size\"] = 3.5 * 1.1\n",
    "plt.rcParams[\"ytick.minor.size\"] = 2 * 1.1\n",
    "# legend\n",
    "legend_cfg = {\"frameon\": False, \"handletextpad\": 0.5}\n",
    "tight_layout_cfg = {\"pad\": 0.001}\n",
    "LG_FRAMEON = False              # no legend frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__CUDNN VERSION: 90100\n",
      "__Number CUDA Devices: 4\n",
      "__CUDA Device Name: Tesla V100-SXM2-16GB\n",
      "__CUDA Device Total Memory [GB]: 16.935419904\n"
     ]
    }
   ],
   "source": [
    "# check for GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print(\"__CUDNN VERSION:\", torch.backends.cudnn.version())\n",
    "    print(\"__Number CUDA Devices:\", torch.cuda.device_count())\n",
    "    print(\"__CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\n",
    "        \"__CUDA Device Total Memory [GB]:\",\n",
    "        torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2s)Load data and evaluate model\n",
    "\n",
    "* training on full dataset\n",
    "* lasso regularized \n",
    "* the dataset is a dataframe that contains the sorted single-units (indices), their quality metrics and their quality label ()\"good\" or \"bad\" units evaluated with our ground truth, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.genmod.generalized_linear_model.GLMResultsWrapper at 0x7ffe4c14d3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "LOAD_DATA = False\n",
    "EVAL = False\n",
    "SAVE_MODEL = False\n",
    "\n",
    "# load the dataset formatted for the model\n",
    "if LOAD_DATA:\n",
    "    data_flc = flc_dataloader.load_dataset(\n",
    "        quality_path, \"E\", \"KS4\", KS4_e_10m, STUDY_e, STUDY_e_su, GT_e_10m\n",
    "    )\n",
    "\n",
    "# evaluate the model\n",
    "if EVAL:\n",
    "\n",
    "    # instantiate the model\n",
    "    flcmodel = FlcModel(data_flc[\"predictors\"])\n",
    "\n",
    "    # train and evaluate with cross-validation\n",
    "    flc_results = flcmodel.evaluate_on_full_dataset(\n",
    "        data_flc[\"dataset\"],\n",
    "        thresh=0.8,\n",
    "        regularization=\"elastic_net\",\n",
    "        maxiter=100,\n",
    "        cnvrg_tol=1e-10,\n",
    "        scale_data=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    display(flc_results[\"metric_data\"])\n",
    "\n",
    "# save model\n",
    "if SAVE_MODEL:\n",
    "    utils.create_if_not_exists(os.path.dirname(model_path))\n",
    "    flc_results[\"metric_data\"][\"model\"].save(model_path)\n",
    "else:\n",
    "    model = load_pickle(model_path)\n",
    "    display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebraspike3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
