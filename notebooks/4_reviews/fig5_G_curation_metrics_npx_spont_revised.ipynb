{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flc-model\n",
    "\n",
    "author: laquitainesteeve@gmail.com\n",
    "\n",
    "purpose: feature weights of classifier to predict \"good single-units\" (>80% agreement score).\n",
    "\n",
    "\n",
    "**Method**\n",
    "- 10 minutes of recording \n",
    "- extract waveforms kilosort4\n",
    "- kilosort4's sorting extractor was generated with rtx5090 GPU, so the results slightly change from the original Tesla V100\n",
    "\n",
    "## Setup \n",
    "\n",
    "activate spikebias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Spikeinterface 0.100.5\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import spikeinterface as si\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.core.template_tools as ttools\n",
    "from spikeinterface import comparison\n",
    "from spikeinterface.curation import remove_excess_spikes\n",
    "from spikeinterface import extract_waveforms\n",
    "from spikeinterface.postprocessing import compute_principal_components\n",
    "from spikeinterface.qualitymetrics import compute_quality_metrics as qm\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.validation import float_like\n",
    "# from cebra import CEBRA\n",
    "# import cebra\n",
    "# import cebra.models\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Spikeinterface\", si.__version__)\n",
    "\n",
    "# set project path\n",
    "proj_path = \"/home/steeve/steeve/epfl/code/spikebias/\"\n",
    "os.chdir(proj_path)\n",
    "\n",
    "from src.nodes.utils import get_config\n",
    "from src.nodes import utils\n",
    "from src.nodes.metrics.quality import get_scores\n",
    "from src.nodes.models.Flc.models import FlcModel # FLC model\n",
    "from src.nodes.models.Flc import dataloader\n",
    "from src.nodes.models.Flc import plotutils as flcplot\n",
    "\n",
    "# setup waveform extraction parameters\n",
    "\n",
    "DURATION_SEC = 600          # 10 minutes\n",
    "CHUNKS = 50000              # (default = 800000) use smaller chunks to use less RAM\n",
    "MAX_SPIKES_PER_UNIT = 500   # (default = 500) max number of spikes used per unit\n",
    "WE_JOB_KWARGS = dict(n_jobs=20, chunk_size=800000, progress_bar=True)\n",
    "job_kwargs = dict(n_jobs=-1, progress_bar=True)\n",
    "\n",
    "# model parameters\n",
    "model_prms = {\"seeds\": np.arange(0,100,1), \"scale_data\": False}\n",
    "\n",
    "# setup paths\n",
    "\n",
    "# npx spont\n",
    "RECORDING_NS = \"dataset/00_raw/recording_npx_spont/\"\n",
    "KS4_ns_10m = \"dataset/01_intermediate/sorting/npx_spont/SortingKS4_10m/\"\n",
    "# GT_ns_10m = cfg_ns[\"sorting\"][\"simulation\"][\"ground_truth\"][\"10m\"][\"output\"] # KS4 sorting\n",
    "GT_ns = \"dataset/00_raw/ground_truth_npx_spont\"\n",
    "GT_ns_10m = \"dataset/00_raw/ground_truth_npx_spont_10m\"\n",
    "# STUDY_ns = cfg_ns[\"postprocessing\"][\"waveform\"][\"sorted\"][\"study\"][\"kilosort4\"][\n",
    "#     \"10m\"\n",
    "# ]  # WaveformExtractor\n",
    "STUDY_ns = \"dataset/01_intermediate/waveforms/npx_spont/SortingKS4_10m/\"\n",
    "# STUDY_ns_su = '/gpfs/bbp.cscs.ch/project/proj85/laquitai/spikebias_paper/0_silico/neuropixels/concatenated_campaigns/postpro/realism/spike/sorted/study_ks4_10m_single_units'\n",
    "STUDY_ns_su = \"dataset/01_intermediate/waveforms/npx_spont/SortingKS4_10m_single_units\"\n",
    "# quality_path = \"/gpfs/bbp.cscs.ch/project/proj85/laquitai/spikebias_paper/analysis/sorting_quality/sorting_quality.csv\"\n",
    "quality_path = \"dataset/01_intermediate/analysis/sorting_quality/sorting_quality.csv\" # sorted unit quality labels\n",
    "\n",
    "# model save path\n",
    "error_path = \"/gpfs/bbp.cscs.ch/project/proj85/laquitai/spikebias_paper/analysis/sorting_quality/models/cebra/sf_40Khz/error_path_s2s\"\n",
    "\n",
    "# figure parameters\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"font.size\"] = 6  # 5-7 with Nature neuroscience as reference\n",
    "plt.rcParams[\"lines.linewidth\"] = 0.5 # typically between 0.5 and 1\n",
    "plt.rcParams[\"axes.linewidth\"] = 0.5 #1\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"xtick.major.width\"] = 0.5 #0.8 #* 1.3\n",
    "plt.rcParams[\"xtick.minor.width\"] = 0.5 #0.8 #* 1.3\n",
    "plt.rcParams[\"ytick.major.width\"] = 0.5 #0.8 #* 1.3\n",
    "plt.rcParams[\"ytick.minor.width\"] = 0.5 #0.8 #* 1.3\n",
    "plt.rcParams[\"xtick.major.size\"] = 3.5 * 1.1\n",
    "plt.rcParams[\"xtick.minor.size\"] = 2 * 1.1\n",
    "plt.rcParams[\"ytick.major.size\"] = 3.5 * 1.1\n",
    "plt.rcParams[\"ytick.minor.size\"] = 2 * 1.1\n",
    "# legend\n",
    "legend_cfg = {\"frameon\": False, \"handletextpad\": 0.5}\n",
    "tight_layout_cfg = {\"pad\": 0.001}\n",
    "LG_FRAMEON = False              # no legend frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__CUDNN VERSION: 90501\n",
      "__Number CUDA Devices: 1\n",
      "__CUDA Device Name: NVIDIA GeForce RTX 5090\n",
      "__CUDA Device Total Memory [GB]: 33.660469248\n"
     ]
    }
   ],
   "source": [
    "# check for GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print(\"__CUDNN VERSION:\", torch.backends.cudnn.version())\n",
    "    print(\"__Number CUDA Devices:\", torch.cuda.device_count())\n",
    "    print(\"__CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\n",
    "        \"__CUDA Device Total Memory [GB]:\",\n",
    "        torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET -----------------------------------------------------------\n",
    "\n",
    "def get_good_sorted_unit_ids(\n",
    "    quality, quality_path: str, sorter: str, exp: str, layer: str, fltd_unit: list\n",
    "):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        quality (_type_): _description_\n",
    "        quality_path (str): _description_\n",
    "        sorter (str): _description_\n",
    "        exp (str): _description_\n",
    "        layer (str): _description_\n",
    "        fltd_unit (list): _description_\n",
    "\n",
    "    Returns:\n",
    "        np.array(int): filtered sorted unit ids\n",
    "    \"\"\"\n",
    "    # load quality results\n",
    "    unit_quality = pd.read_csv(quality_path)\n",
    "\n",
    "    # select a sorted unit and conditions\n",
    "    df = unit_quality[\n",
    "        (unit_quality[\"quality\"].str.contains(quality))\n",
    "        & (unit_quality[\"experiment\"] == exp)\n",
    "        & (unit_quality[\"sorter\"] == sorter)\n",
    "        & (unit_quality[\"layer\"] == layer)\n",
    "    ]\n",
    "    # filter units based on previous conditions\n",
    "    df = df[df[\"sorted\"].isin(fltd_unit)]\n",
    "    return df[\"sorted\"].values.astype(int)\n",
    "\n",
    "\n",
    "def get_poor_sorted_unit_ids(\n",
    "    quality, quality_path: str, sorter: str, exp: str, layer: str, fltd_unit: list\n",
    "):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        quality (_type_): _description_\n",
    "        quality_path (str): _description_\n",
    "        sorter (str): _description_\n",
    "        exp (str): _description_\n",
    "        layer (str): _description_\n",
    "        fltd_unit (list[int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        np.array[int]: filtered sorted unit ids\n",
    "    \"\"\"\n",
    "    np.array(fltd_unit).astype(int)\n",
    "\n",
    "    # load quality results\n",
    "    unit_quality = pd.read_csv(quality_path)\n",
    "\n",
    "    # select a sorted unit and conditions\n",
    "    df = unit_quality[\n",
    "        (unit_quality[\"quality\"] == quality)\n",
    "        & (unit_quality[\"experiment\"] == exp)\n",
    "        & (unit_quality[\"sorter\"] == sorter)\n",
    "        & (unit_quality[\"layer\"] == layer)\n",
    "    ]\n",
    "    df = df[df[\"sorted\"].isin(fltd_unit)]\n",
    "    return df[\"sorted\"].values.astype(int)\n",
    "\n",
    "\n",
    "def get_spike_dataset_for(\n",
    "    unit_ids: np.array,\n",
    "    we,\n",
    "    max_spikes: int,\n",
    "    interval_ms: float,\n",
    "    sfreq: int,\n",
    "    downsample: int,\n",
    "):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        unit_ids (np.array[int]): _description_\n",
    "        we (_type_): _description_\n",
    "        max_spikes (int): _description_\n",
    "        interval_ms (float): _description_\n",
    "        sfreq (int): _description_\n",
    "        downsample (int): downsample waveforms to produce\n",
    "        - a lower sampling frequency (e.g., 2 to reduce a 40 KHz frequency\n",
    "        to 20 KHz)\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # convert interval in ms to samples\n",
    "    ttp_sp = we.nbefore\n",
    "    bef_aft_sp = interval_ms * sfreq / 1000\n",
    "    interval = np.arange(ttp_sp - bef_aft_sp, ttp_sp + bef_aft_sp, 1).astype(int)\n",
    "\n",
    "    # get all units nearest channels (with extremum amplitude)\n",
    "    max_chids = ttools.get_template_extremum_channel(we, peak_sign=\"both\")\n",
    "\n",
    "    # loop over good units\n",
    "    # 240 samples (-3 to 3 ms at 40KHz)\n",
    "    wvs = np.zeros((int(max_spikes * we.nbefore * 2 / downsample), 1))\n",
    "    unit_label = []\n",
    "    for unit in unit_ids:\n",
    "\n",
    "        # get its waveforms (num_spikes, num_samples, num_channels)\n",
    "        wv = we.get_waveforms(unit_id=unit)\n",
    "\n",
    "        # get its nearest channel\n",
    "        c_ids = we.sparsity.unit_id_to_channel_ids[unit]\n",
    "        max_chid = max_chids[unit]\n",
    "        max_chid_ix = np.where(c_ids == max_chid)[0][0]\n",
    "\n",
    "        # get waveform for that channels (2D)\n",
    "        # and (num_samples, num_spikes)\n",
    "        # TODO: sample instead of taking the first ones\n",
    "        wv_i = np.array(wv[:max_spikes, interval[::downsample], max_chid_ix]).flatten()[\n",
    "            :, None\n",
    "        ]\n",
    "\n",
    "        # record waveforms\n",
    "        wvs = np.hstack([wvs, wv_i])\n",
    "\n",
    "    wvs = wvs[:, 1:]\n",
    "    # unit_label = np.array(unit_label)\n",
    "    unit_label = np.array(unit_ids)\n",
    "    return wvs, unit_label\n",
    "\n",
    "\n",
    "def get_sorted_unit_best_score(\n",
    "    KS4_ns_10m: str,\n",
    "    GT_ns_10m: str,\n",
    "    sfreq:int\n",
    "):\n",
    "    \"\"\"Get sorted unit best agreement scores\n",
    "\n",
    "    Args:\n",
    "        KS4_ns_10m (str): path of SortingExtractor\n",
    "        GT_ns_10m (str): path of GroundTruth SortingExtractor\n",
    "\n",
    "    Returns:\n",
    "        pd.Series:\n",
    "        - index_ sorted units\n",
    "        - values: agreement scores\n",
    "    \"\"\"\n",
    "    # load \n",
    "    SortingTrue = si.load_extractor(GT_ns_10m)\n",
    "    \n",
    "    # get 10 mins\n",
    "    SortingTrue = SortingTrue.frame_slice(start_frame=0, end_frame=10 * 60 * sfreq)\n",
    "\n",
    "    # remove empty units\n",
    "    SortingTrue = SortingTrue.remove_empty_units()\n",
    "\n",
    "    # load sorting\n",
    "    Sorting = si.load_extractor(KS4_ns_10m)\n",
    "\n",
    "    # compute agreement score\n",
    "    comp = comparison.compare_sorter_to_ground_truth(\n",
    "        SortingTrue,\n",
    "        Sorting,\n",
    "        match_mode=\"hungarian\",\n",
    "        exhaustive_gt=True,\n",
    "        delta_time=1.3,\n",
    "        compute_labels=True,\n",
    "        compute_misclassifications=False,\n",
    "        well_detected_score=0.8,\n",
    "        match_score=0.8,  # modified\n",
    "        redundant_score=0.2,  # default - we don't use that info in this analysis\n",
    "        overmerged_score=0.2,  # default - we don't use that info in this analysis\n",
    "        chance_score=0.1,  # default - we don't use that info in this analysis\n",
    "    )\n",
    "    return comp.agreement_scores.max()\n",
    "\n",
    "\n",
    "def get_dataset_for(\n",
    "    we,\n",
    "    quality_path: str,\n",
    "    sorter: str,\n",
    "    exp: str,\n",
    "    layer: str,\n",
    "    max_spikes: int,\n",
    "    flt_unit: list[int],\n",
    "    interval_ms: float,\n",
    "    sfreq: int,\n",
    "    downsample: int,\n",
    "):\n",
    "    \"\"\"get dataset\n",
    "\n",
    "    Args:\n",
    "        we (WaveformExtractor): WaveformExtractor\n",
    "        quality_path (str): path of the pandas dataframe\n",
    "        - containing sorted unit quality classification\n",
    "        sorter (str): one of \"KS4\", \"KS3\", \"KS2.5\", \"KS2\"...\n",
    "        - contained in the quality dataframe in the \"sorter\" column\n",
    "        exp (str): _description_\n",
    "        layer (str): _description_\n",
    "        max_spikes (int): _description_\n",
    "        flt_unit (list[int]): _description_\n",
    "        interval_ms (float): _description_\n",
    "        sfreq (int): _description_\n",
    "        downsample\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # get good units (filtered based on some conditions)\n",
    "    g_units = get_good_sorted_unit_ids(\n",
    "        \"good\", quality_path, sorter, exp, layer, flt_unit\n",
    "    )\n",
    "    wvs_good, good_unit_label = get_spike_dataset_for(\n",
    "        g_units, we, max_spikes, interval_ms, sfreq, downsample\n",
    "    )\n",
    "\n",
    "    # poor units (filtered based on some conditions)\n",
    "    p_units = get_poor_sorted_unit_ids(\n",
    "        \"mixed: overmerger + oversplitter\", quality_path, sorter, exp, layer, flt_unit\n",
    "    )\n",
    "    wvs_poor, poor_unit_label = get_spike_dataset_for(\n",
    "        p_units, we, max_spikes, interval_ms, sfreq, downsample\n",
    "    )\n",
    "\n",
    "    # spike dataset\n",
    "    spike_data = np.hstack([wvs_good, wvs_poor]).T\n",
    "\n",
    "    # quality label (1D discrete, CEBRA can handle only one)\n",
    "    quality_label = np.hstack(\n",
    "        [np.array([1] * len(good_unit_label)), np.array([0] * len(poor_unit_label))]\n",
    "    )\n",
    "\n",
    "    # unit ids\n",
    "    unit_ids = np.hstack([g_units, p_units])\n",
    "\n",
    "    return spike_data, quality_label, unit_ids\n",
    "\n",
    "\n",
    "def get_dataset_by_layer(\n",
    "    sort_path: str,\n",
    "    gt_path: str,\n",
    "    study: str,\n",
    "    quality_path: str,\n",
    "    sorter: str,\n",
    "    exp: str,\n",
    "    num_spike: int,\n",
    "    interval_ms: float,\n",
    "    downsample: int,\n",
    "):\n",
    "    \"\"\"get a dataset by layer\n",
    "\n",
    "    Args:\n",
    "        sort_path (str): _description_\n",
    "        gt_path (str): ground truth SortingExtractor\n",
    "        STUDY (str): _description_\n",
    "        quality_path (str): _description_\n",
    "        sorter (str): _description_\n",
    "        exp (str): _description_\n",
    "        num_spike (int): _description_\n",
    "        interval_ms (float): _description_\n",
    "        downsample (int): can be a factor of 1, 2, 3 ...\n",
    "\n",
    "    Returns:\n",
    "        dict: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate the common maximum number\n",
    "    # of spikes that it is possible to use\n",
    "    # across all units (the number of spikes of the\n",
    "    # least active unit)\n",
    "    Sorting = si.load_extractor(sort_path)\n",
    "    unit_spike = Sorting.get_total_num_spikes()\n",
    "    sfreq = Sorting.get_sampling_frequency()\n",
    "    print(\"Recording sampling frequency:\", sfreq)\n",
    "\n",
    "    # select unit ids with at least num_spikes\n",
    "    n_spike = [unit_spike[key] for key in unit_spike]\n",
    "    if num_spike == None:\n",
    "        num_spike = min(n_spike)\n",
    "        flt_unit = [unit for unit in unit_spike]\n",
    "    else:\n",
    "        flt_unit = [unit for unit in unit_spike if unit_spike[unit] > num_spike]\n",
    "\n",
    "    # get waveformExtractor\n",
    "    we = si.WaveformExtractor.load_from_folder(study)\n",
    "\n",
    "    # get data for CEBRA by layer\n",
    "    # L2/3\n",
    "    # spike_data_l23, quality_label_l23, unit_ids_l23 = get_dataset_for(\n",
    "    #     we,\n",
    "    #     quality_path,\n",
    "    #     sorter,\n",
    "    #     exp,\n",
    "    #     \"L2/3\",\n",
    "    #     num_spike,\n",
    "    #     flt_unit,\n",
    "    #     interval_ms,\n",
    "    #     sfreq,\n",
    "    #     downsample,\n",
    "    # )\n",
    "    # L4\n",
    "    spike_data_l4, quality_label_l4, unit_ids_l4 = get_dataset_for(\n",
    "        we,\n",
    "        quality_path,\n",
    "        sorter,\n",
    "        exp,\n",
    "        \"L4\",\n",
    "        num_spike,\n",
    "        flt_unit,\n",
    "        interval_ms,\n",
    "        sfreq,\n",
    "        downsample,\n",
    "    )\n",
    "    # L5\n",
    "    spike_data_l5, quality_label_l5, unit_ids_l5 = get_dataset_for(\n",
    "        we,\n",
    "        quality_path,\n",
    "        sorter,\n",
    "        exp,\n",
    "        \"L5\",\n",
    "        num_spike,\n",
    "        flt_unit,\n",
    "        interval_ms,\n",
    "        sfreq,\n",
    "        downsample,\n",
    "    )\n",
    "    # L6\n",
    "    spike_data_l6, quality_label_l6, unit_ids_l6 = get_dataset_for(\n",
    "        we,\n",
    "        quality_path,\n",
    "        sorter,\n",
    "        exp,\n",
    "        \"L6\",\n",
    "        num_spike,\n",
    "        flt_unit,\n",
    "        interval_ms,\n",
    "        sfreq,\n",
    "        downsample,\n",
    "    )\n",
    "\n",
    "    # get best scores of sorted unit\n",
    "    best_score = get_sorted_unit_best_score(\n",
    "        sort_path,\n",
    "        gt_path,\n",
    "    )\n",
    "    print(\"ex. data shape (L4):\", spike_data_l4.shape)\n",
    "    print(\"ex. label shape (L4):\", quality_label_l4.shape)\n",
    "\n",
    "    # bundle dataset for model 1 (by layer)\n",
    "    dataset1 = {\n",
    "        # \"data_l23\": spike_data_l23,\n",
    "        \"data_l4\": spike_data_l4,\n",
    "        \"data_l5\": spike_data_l5,\n",
    "        \"data_l6\": spike_data_l6,\n",
    "        # \"label_l23\": quality_label_l23,\n",
    "        \"label_l4\": quality_label_l4,\n",
    "        \"label_l5\": quality_label_l5,\n",
    "        \"label_l6\": quality_label_l6,\n",
    "        # \"unit_ids_l23\": unit_ids_l23,\n",
    "        \"unit_ids_l4\": unit_ids_l4,\n",
    "        \"unit_ids_l5\": unit_ids_l5,\n",
    "        \"unit_ids_l6\": unit_ids_l6,\n",
    "        \"best_score\": best_score,\n",
    "        \"nb_spikes\": num_spike,\n",
    "    }\n",
    "    return dataset1\n",
    "\n",
    "\n",
    "def get_dataset_pooled(dat1):\n",
    "\n",
    "    # spike_data = np.vstack(\n",
    "    #     [dat1[\"data_l23\"], dat1[\"data_l4\"], dat1[\"data_l5\"], dat1[\"data_l6\"]]\n",
    "    # )\n",
    "    # quality_label = np.hstack(\n",
    "    #     [dat1[\"label_l23\"], dat1[\"label_l4\"], dat1[\"label_l5\"], dat1[\"label_l6\"]]\n",
    "    # )\n",
    "    # unit_ids = np.hstack(\n",
    "    #     [\n",
    "    #         dat1[\"unit_ids_l23\"],\n",
    "    #         dat1[\"unit_ids_l4\"],\n",
    "    #         dat1[\"unit_ids_l5\"],\n",
    "    #         dat1[\"unit_ids_l6\"],\n",
    "    #     ]\n",
    "    # )\n",
    "    spike_data = np.vstack([dat1[\"data_l4\"], dat1[\"data_l5\"], dat1[\"data_l6\"]])\n",
    "    quality_label = np.hstack([dat1[\"label_l4\"], dat1[\"label_l5\"], dat1[\"label_l6\"]])\n",
    "    unit_ids = np.hstack(\n",
    "        [\n",
    "            dat1[\"unit_ids_l4\"],\n",
    "            dat1[\"unit_ids_l5\"],\n",
    "            dat1[\"unit_ids_l6\"],\n",
    "        ]\n",
    "    )\n",
    "    return {\"data\": spike_data, \"label\": quality_label, \"unit_ids\": unit_ids}\n",
    "\n",
    "\n",
    "def vanilla_cv_split(label, split, seed):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # get good unit indices\n",
    "    good_ix = np.where(label)[0]\n",
    "    n_tr = int(np.floor(len(good_ix) * split))\n",
    "    shuffled = np.random.permutation(good_ix)\n",
    "    g_tr_ix = shuffled[:n_tr]\n",
    "\n",
    "    # get poor unit indices\n",
    "    poor_ix = np.where(label == 0)[0]\n",
    "    n_tr = int(np.floor(len(poor_ix) * split))\n",
    "    shuffled = np.random.permutation(poor_ix)\n",
    "    p_tr_ix = shuffled[:n_tr]\n",
    "\n",
    "    # get train and test indices\n",
    "    tr_ix = np.hstack([g_tr_ix, p_tr_ix])\n",
    "    all_ix = np.arange(0, len(label), 1)\n",
    "    test_ix = np.where(~np.isin(all_ix, tr_ix))[0]\n",
    "    return tr_ix, test_ix\n",
    "\n",
    "\n",
    "def plot_em(ax, CebraL4_em, quality_label, xlim):\n",
    "    \"\"\"plot the embedding, on which dots are\n",
    "    sorted units colored by sorting quality\n",
    "    (good in \"red\", poor in \"black\")\n",
    "\n",
    "    Args:\n",
    "        ax (_type_): axis\n",
    "        CebraL4_em (np.array): embedding\n",
    "        quality_label (np.array): quality labels\n",
    "        - (1: good, 0: poor)\n",
    "\n",
    "    Returns:\n",
    "        scat: plot handle\n",
    "    \"\"\"\n",
    "    # set color for good units in red, poor in black\n",
    "    colr = np.array([\"None\"] * len(quality_label))\n",
    "    colr[quality_label == 1] = \"r\"\n",
    "    colr[quality_label == 0] = \"k\"\n",
    "\n",
    "    # plot\n",
    "    ax.view_init(20, 45, 0)  # elevation, azimuth, roll\n",
    "    scat = ax.scatter(\n",
    "        CebraL4_em[:, 0],\n",
    "        CebraL4_em[:, 1],\n",
    "        CebraL4_em[:, 2],\n",
    "        c=colr,\n",
    "        edgecolors=\"w\",\n",
    "        linewidths=0.2,\n",
    "        s=20,\n",
    "    )\n",
    "    # aesthetics\n",
    "    # disconnect axes (R style)\n",
    "    ax.spines[[\"right\", \"top\"]].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_position((\"axes\", -0.05))\n",
    "    ax.spines[\"left\"].set_position((\"axes\", -0.05))\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(xlim)\n",
    "    ax.set_zlim(xlim)\n",
    "    return scat\n",
    "\n",
    "\n",
    "def plot_model1_em_by_layer(fig, dataset, em, xlim):\n",
    "\n",
    "    # L2/3\n",
    "    # ax = fig.add_subplot(1, 4, 1, projection=\"3d\")\n",
    "    # scat = plot_em(ax, em[\"l23\"], dataset[\"label_l23\"], xlim)\n",
    "    # ax.set_title(\"L2/3\")\n",
    "    # L4\n",
    "    # ax = fig.add_subplot(1, 4, 2, projection=\"3d\")\n",
    "    ax = fig.add_subplot(1, 3, 1, projection=\"3d\")\n",
    "    scat = plot_em(ax, em[\"l4\"], dataset[\"label_l4\"], xlim)\n",
    "    ax.set_title(\"L4\")\n",
    "    # L5\n",
    "    # ax = fig.add_subplot(1, 4, 3, projection=\"3d\")\n",
    "    ax = fig.add_subplot(1, 3, 2, projection=\"3d\")\n",
    "    scat = plot_em(ax, em[\"l5\"], dataset[\"label_l5\"], xlim)\n",
    "    ax.set_title(\"L5\")\n",
    "    # L6\n",
    "    # ax = fig.add_subplot(1, 4, 4, projection=\"3d\")\n",
    "    ax = fig.add_subplot(1, 3, 3, projection=\"3d\")\n",
    "    scat = plot_em(ax, em[\"l6\"], dataset[\"label_l6\"], xlim)\n",
    "    ax.set_title(\"L6\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_model2_em_by_layer(fig, dataset, em, xlim):\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "    scat = plot_em(ax, em, dataset[\"label\"], xlim)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def decode(embed_train, embed_test, label_train, label_test):\n",
    "    \"\"\"decoding using a k-Nearest Neighbor clustering technique\n",
    "    We use the fixed number of neighbors 2\n",
    "    \"\"\"\n",
    "    # predict\n",
    "    decoder = cebra.KNNDecoder(n_neighbors=2, metric=\"cosine\")\n",
    "\n",
    "    # train kNN on training embedding\n",
    "    decoder.fit(embed_train, label_train)\n",
    "\n",
    "    # decode test embedding\n",
    "    prediction = decoder.predict(embed_test)\n",
    "\n",
    "    # calculate performance metrics\n",
    "    # precision and recall are for label 1 (\"good\" units)\n",
    "    accuracy = sklearn.metrics.accuracy_score(label_test, prediction)\n",
    "    bal_accuracy = sklearn.metrics.balanced_accuracy_score(label_test, prediction)\n",
    "    precision = sklearn.metrics.precision_score(label_test, prediction, pos_label=1)\n",
    "    recall = sklearn.metrics.recall_score(label_test, prediction, pos_label=1)\n",
    "    f1_score = sklearn.metrics.f1_score(label_test, prediction, pos_label=1)\n",
    "    mae = np.median(abs(prediction - label_test))\n",
    "    r2 = sklearn.metrics.r2_score(label_test, prediction)\n",
    "    return {\n",
    "        \"metrics\": {\n",
    "            \"mae\": mae,\n",
    "            \"r2\": r2,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"bal_accuracy\": bal_accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score,\n",
    "        },\n",
    "        \"prediction\": prediction,\n",
    "    }\n",
    "\n",
    "\n",
    "# UNIT QUALITY METRICS ---------------------------------------\n",
    "\n",
    "\n",
    "def get_waveformExtractor_for_single_units(\n",
    "    sort_path: str,\n",
    "    study_path,\n",
    "    save_path: str,\n",
    "    n_sites=384,\n",
    "    load_if_exists: bool = False,\n",
    "    add_pca: bool = True,\n",
    "    n_components=5,\n",
    "):\n",
    "    \"\"\"Setup WaveformExtractors to calculate quality metrics for single units\n",
    "\n",
    "    Args:\n",
    "        sort_path (str): _description_\n",
    "        study_path (_type_): _description_\n",
    "        save_path (str): _description_\n",
    "        n_sites (int, optional): _description_. Defaults to 384.\n",
    "        load_if_exists: bool=False (bool)\n",
    "        add_pca (bool): only if load_if_exists=False\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # compute\n",
    "    if not load_if_exists:\n",
    "        \n",
    "        # get single units\n",
    "        Sorting = si.load_extractor(sort_path)\n",
    "        su_ix = np.where(Sorting.get_property(\"KSLabel\") == \"good\")[0]\n",
    "        su_unit_ids = Sorting.unit_ids[su_ix]\n",
    "\n",
    "        # load WaveformExtractor\n",
    "        We = si.WaveformExtractor.load_from_folder(study_path)\n",
    "\n",
    "        # create waveformExtractor for single units\n",
    "        # which we will keep for all downstream analyses\n",
    "        # this should speed up computations\n",
    "        shutil.rmtree(save_path, ignore_errors=True)\n",
    "        WeSu = We.select_units(unit_ids=su_unit_ids, new_folder=save_path)\n",
    "\n",
    "        # setup two properties required to calculate some quality metrics\n",
    "        WeSu.recording.set_property(\"gain_to_uV\", np.ones((n_sites,)))\n",
    "        WeSu.recording.set_property(\"offset_to_uV\", np.zeros((n_sites,)))\n",
    "\n",
    "        # augment extractors with pca results\n",
    "        if add_pca:\n",
    "            _ = compute_principal_components(\n",
    "                waveform_extractor=WeSu,\n",
    "                n_components=n_components,\n",
    "                mode=\"by_channel_local\",\n",
    "                **job_kwargs,\n",
    "            )\n",
    "    else:\n",
    "        # or load existing\n",
    "        WeSu = si.WaveformExtractor.load_from_folder(save_path)\n",
    "    return WeSu\n",
    "\n",
    "\n",
    "def add_spike_amplitude_extension(we, n_sites, load_if_exists: bool):\n",
    "    \"\"\"Add spike amplitudes to WaveformExtractor\n",
    "\n",
    "    Args:\n",
    "        we (WaveformExtractor): _description_\n",
    "        n_sites (int): typically 384 for neuropixels\n",
    "        load_if_exists (bool): load if exists\n",
    "\n",
    "    Returns:\n",
    "        WaveformExtractor: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # these two properties are required to compute amplitudes\n",
    "    we.recording.set_property(\"gain_to_uV\", np.ones((n_sites,)))\n",
    "    we.recording.set_property(\"offset_to_uV\", np.zeros((n_sites,)))\n",
    "\n",
    "    # compute spike amplitudes\n",
    "    # or it as an extension\n",
    "    if not load_if_exists:\n",
    "        _ = si.postprocessing.compute_spike_amplitudes(we, outputs=\"by_unit\")\n",
    "    else:\n",
    "        we.load_extension(\"spike_amplitudes\")\n",
    "\n",
    "    # unit-test\n",
    "    assert we.has_extension(\"spike_amplitudes\"), \"load spike_amplitudes extension\"\n",
    "    return we\n",
    "\n",
    "\n",
    "def get_quality_metrics(KS4_ns_10m, STUDY_ns, STUDY_ns_su, n_sites, load_if_exists):\n",
    "\n",
    "    # (40s)for single units\n",
    "    # note: adding PCA takes 3 hours (do once, then set load_if_exists=True)\n",
    "    WeNs = get_waveformExtractor_for_single_units(\n",
    "        KS4_ns_10m, STUDY_ns, STUDY_ns_su, n_sites=384, load_if_exists=True\n",
    "    )\n",
    "    # add spike amplitudes\n",
    "    WeNs = add_spike_amplitude_extension(WeNs, n_sites=384, load_if_exists=True)\n",
    "\n",
    "    # pre-compute Spiketinterface quality metrics\n",
    "    # 20 secs/unit\n",
    "    qmetrics = qm(\n",
    "        WeNs,\n",
    "        qm_params={\n",
    "            \"amplitude_cutoff\": {\n",
    "                \"peak_sign\": \"neg\",\n",
    "                \"num_histogram_bins\": 100,\n",
    "                \"histogram_smoothing_value\": 3,\n",
    "                \"amplitudes_bins_min_ratio\": 0,  # instead of 5\n",
    "            }\n",
    "        },\n",
    "        load_if_exists=load_if_exists,\n",
    "        skip_pc_metrics=True,\n",
    "        **job_kwargs,\n",
    "    )\n",
    "    qmetrics = qmetrics[\n",
    "        [\n",
    "            \"amplitude_cutoff\",\n",
    "            \"firing_range\",\n",
    "            \"firing_rate\",\n",
    "            \"isi_violations_ratio\",\n",
    "            \"presence_ratio\",\n",
    "            \"rp_contamination\",\n",
    "            \"rp_violations\",\n",
    "            \"sd_ratio\",\n",
    "            \"snr\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # add silhouette metric (pca-based but fast enough)\n",
    "    silhouette = qm(\n",
    "        WeNs,\n",
    "        metric_names=[\"silhouette\"],\n",
    "        **job_kwargs,\n",
    "    )\n",
    "    qmetrics[\"silhouette\"] = silhouette.values\n",
    "\n",
    "    # handle missing metrics\n",
    "    print(\"****************** Analysing data completion ***************\")\n",
    "\n",
    "    print(\"Data completion:\", qmetrics.notna().sum())\n",
    "\n",
    "    print(\"quality metrics are:\", qmetrics.columns)\n",
    "\n",
    "    return qmetrics\n",
    "\n",
    "\n",
    "def get_best_site_mad_noise(we, max_chids, unit):\n",
    "\n",
    "    # get waveforms\n",
    "    wv, _ = we.get_waveforms(unit_id=unit, with_index=True)\n",
    "\n",
    "    # get channel ids (sparse)\n",
    "    c_ids = we.sparsity.unit_id_to_channel_ids[unit]\n",
    "\n",
    "    # get nearest channel\n",
    "    max_chid = max_chids[unit]\n",
    "    max_chid_ix = np.where(c_ids == max_chid)[0][0]\n",
    "    return wv[:, :, max_chid_ix].flatten()\n",
    "\n",
    "\n",
    "def mad(data):\n",
    "    mean_data = np.mean(data)\n",
    "    return np.mean(np.absolute(data - mean_data))\n",
    "\n",
    "\n",
    "def get_mad_ratio(spike_amp, noise_amp):\n",
    "    \"\"\"calculate an sd_ratio robust to outliers\n",
    "\n",
    "    Args:\n",
    "        spike_amp (_type_): _description_\n",
    "        noise_amp (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    mad_unit = mad(spike_amp)  # twice smaller than std\n",
    "    mad_noise = mad(noise_amp)\n",
    "    return mad_unit / mad_noise\n",
    "\n",
    "\n",
    "def get_mad_ratio_all_units(unit_ids, WeNS, spike_amp):\n",
    "    max_chids = ttools.get_template_extremum_channel(WeNS, peak_sign=\"both\")\n",
    "    mad_ratio = []\n",
    "    for unit in unit_ids:\n",
    "        noise_amp = get_best_site_mad_noise(WeNS, max_chids, unit)\n",
    "        mad_ratio.append(get_mad_ratio(spike_amp[unit], noise_amp))\n",
    "    return mad_ratio\n",
    "\n",
    "\n",
    "def load_results(file_path):\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        predictions_all = pickle.load(input_file)\n",
    "    return predictions_all\n",
    "\n",
    "\n",
    "# MODEL ------------------------------------\n",
    "\n",
    "\n",
    "def loglike(fit_output, params, scale: float, exog: np.ndarray, endog: np.array):\n",
    "    \"\"\"Calculate the log likelihood of observing the true sorting accuracies \"endog\"\n",
    "    given the fitted glm model \"fit_output\"\n",
    "    you can get by inspecting result_1.model.loglike\n",
    "    code = inspect.getsource(result_1.model.loglike)\n",
    "    print(code)\n",
    "\n",
    "    Args:\n",
    "        fit_output: fitted glm model\n",
    "        exog: predictive features used to make predictions\n",
    "        - independent variables\n",
    "        endog: true sorting accuracy (dependent variable)\n",
    "\n",
    "    Note:\n",
    "        Setting the args as below should produce llf == fit_output.llf\n",
    "        that is the log likelihood nproduced from fitting the training\n",
    "        data\n",
    "        - exog = fit_output.model.exog\n",
    "        - endog = fit_output.model.endog\n",
    "    \"\"\"\n",
    "    scale = float_like(scale, \"scale\", optional=True)\n",
    "    var_weights = np.ones(exog.shape[0])\n",
    "    freq_weights = np.ones(exog.shape[0])\n",
    "\n",
    "    # make predictions\n",
    "    # - same as calling result.model.predict(params, exog)\n",
    "    linear_preds = np.dot(exog, params) + fit_output.model._offset_exposure\n",
    "    expval = fit_output.model.family.link.inverse(linear_preds)\n",
    "    if scale is None:\n",
    "        scale = fit_output.model.estimate_scale(expval)\n",
    "\n",
    "    # calculate loglikelihood of data\n",
    "    llf = fit_output.model.family.loglike(\n",
    "        endog,  # true sorting accuracy\n",
    "        expval,  # predicted sorting accuracy\n",
    "        var_weights,  # 1 by default\n",
    "        freq_weights,  # 1 by default\n",
    "        scale,\n",
    "    )\n",
    "    return llf\n",
    "\n",
    "\n",
    "def get_single_fold_mcf_r2(\n",
    "    model_formula: str,\n",
    "    dataset: pd.DataFrame,\n",
    "    split_ratio: float = 0.75,\n",
    "    seed: int = 0,\n",
    "    scale_data=False,\n",
    "):\n",
    "    \"\"\"Calculate mcfadden pseudo r-squared for a single fold, sampling\n",
    "    split_ratio instances of the dataset as train and 1-split_ratio as test\n",
    "\n",
    "    Args:\n",
    "        model_formula\n",
    "\n",
    "    note:\n",
    "        - mcfadden r2 formula: (1 - result_1.llf / result_1.llnull)\n",
    "        - produced by statsmodel r2 = test_model.pseudo_rsquared(kind=\"mcf\")\n",
    "\n",
    "    Returns:\n",
    "        mcfadden pseudo r-squared (float)\n",
    "    \"\"\"\n",
    "    # GET MCF R2 FOR TEST MODEL\n",
    "    random.seed(seed)\n",
    "\n",
    "    # TRAIN -----------\n",
    "    # calculate 75% of train\n",
    "    n_train = np.round(split_ratio * dataset.shape[0]).astype(int)\n",
    "\n",
    "    # sample n_train\n",
    "    indices = np.arange(0, dataset.shape[0], 1).tolist()\n",
    "    train_indices = random.sample(indices, n_train)\n",
    "    train_dataset = dataset.iloc[train_indices, :]\n",
    "\n",
    "    assert (\n",
    "        not \"quality_label\" in dataset.columns\n",
    "    ), \"should drop quality_label from dataset\"\n",
    "\n",
    "    # apply scaling\n",
    "    if scale_data:\n",
    "        standard_scaler = StandardScaler()\n",
    "        predictors = dataset.columns.tolist()\n",
    "        predictors.remove(\"sorting_accuracy\")\n",
    "        train_dataset[predictors] = standard_scaler.fit_transform(\n",
    "            train_dataset[predictors]\n",
    "        )\n",
    "\n",
    "    # train model on this fold\n",
    "    try:\n",
    "        model_1 = sm.GLM.from_formula(\n",
    "            model_formula,\n",
    "            family=sm.families.Binomial(),\n",
    "            data=train_dataset,\n",
    "        )\n",
    "        result_1 = model_1.fit()\n",
    "    except:\n",
    "        raise ValueError(\"Model formula is wrong\")\n",
    "\n",
    "    # TEST -----------\n",
    "    # create test dataset with remaining instances\n",
    "    test_indices = list(set(indices) - set(train_indices))\n",
    "    test_dataset = dataset.iloc[test_indices, :]\n",
    "\n",
    "    # apply scaling\n",
    "    if scale_data:\n",
    "        test_dataset[predictors] = standard_scaler.transform(test_dataset[predictors])\n",
    "\n",
    "    # reorder test dataset features and add intercept\n",
    "    # to make predictions and get loglikelihood\n",
    "    features = result_1.params.index[1:]\n",
    "    test_features = test_dataset.loc[:, features]\n",
    "    test_features.insert(0, \"intercept\", 1)\n",
    "\n",
    "    # test and eval\n",
    "    llf = loglike(\n",
    "        result_1,\n",
    "        result_1.params,\n",
    "        None,\n",
    "        exog=test_features,\n",
    "        endog=test_dataset[\"sorting_accuracy\"],\n",
    "    )\n",
    "\n",
    "    # GET MCF R2 FOR NULL MODEL\n",
    "\n",
    "    # train\n",
    "    null_model = sm.GLM.from_formula(\n",
    "        \"sorting_accuracy ~ 1\",\n",
    "        family=sm.families.Binomial(),\n",
    "        data=train_dataset,\n",
    "    )\n",
    "    null_model = null_model.fit()\n",
    "\n",
    "    # test and eval\n",
    "    ll_null = loglike(\n",
    "        null_model,\n",
    "        null_model.params,\n",
    "        None,\n",
    "        exog=np.array([test_features[\"intercept\"]]).T,\n",
    "        endog=test_dataset[\"sorting_accuracy\"],\n",
    "    )\n",
    "\n",
    "    # fix r-squared in case ll_null==0\n",
    "    if llf > 0 and ll_null == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return 1 - llf / ll_null\n",
    "\n",
    "\n",
    "def get_crossval_mcf_r2(\n",
    "    dataset: pd.DataFrame,\n",
    "    model_formula: str,\n",
    "    split_ratio: float = 0.75,\n",
    "    seeds: np.array = np.arange(0, 100, 1),\n",
    "    scale_data=False,\n",
    "):\n",
    "    \"\"\"Calculate cross-validated mcfadden pseudo r-squared\n",
    "    on test dataset\n",
    "\n",
    "    Args:\n",
    "        model_formula (str): glm model formula\n",
    "        split_ratio (float, optional): _description_. Defaults to 0.75.\n",
    "        seeds (np.array, optional): _description_. Defaults to np.arange(0, 100, 1).\n",
    "\n",
    "    Returns:\n",
    "        np.array: mcfadden pseudo r-squared\n",
    "    \"\"\"\n",
    "    r2_all = []\n",
    "    for seed in range(len(seeds)):\n",
    "        r2 = get_single_fold_mcf_r2(\n",
    "            model_formula,\n",
    "            dataset,\n",
    "            split_ratio=split_ratio,\n",
    "            seed=seed,\n",
    "            scale_data=scale_data,\n",
    "        )\n",
    "        # print(\n",
    "        #     \"Nan because SVD did not converge in Linear Least Squares because of wrong covariance matrix()\"\n",
    "        # )\n",
    "        # # can fail if SVD did not converge in Linear Least Squares\n",
    "        # # because of wrong covariance matrix\n",
    "        # r2 = np.nan\n",
    "        r2_all.append(r2)\n",
    "    return np.array(r2_all)\n",
    "\n",
    "\n",
    "def get_single_fold_flclassifier_metrics(\n",
    "    model_formula: str,\n",
    "    dataset: pd.DataFrame,\n",
    "    split_ratio: float = 0.75,\n",
    "    seed: int = 0,\n",
    "    thresh: float = 0.8,\n",
    "    scale_data=False,\n",
    "):\n",
    "    \"\"\"Calculate the cross-validated precisions\n",
    "    and recalls of a fractional logistic classifier\n",
    "    trained to predict high-quality unit label-\n",
    "\n",
    "    Args:\n",
    "        model_formula\n",
    "\n",
    "    Returns:\n",
    "        precisions and recalls for each fold\n",
    "    \"\"\"\n",
    "    # GET MCF R2 FOR TEST MODEL\n",
    "    random.seed(seed)\n",
    "\n",
    "    # unit-test\n",
    "    # this is a ground truth label that should not\n",
    "    # be in the dataset\n",
    "    assert (\n",
    "        not \"sorting_accuracy\" in dataset.columns\n",
    "    ), \"drop sorting_accuracy from dataset\"\n",
    "\n",
    "    # TRAIN -----------\n",
    "    # calculate 75% of train\n",
    "    n_train = np.round(split_ratio * dataset.shape[0]).astype(int)\n",
    "\n",
    "    # sample n_train\n",
    "    indices = np.arange(0, dataset.shape[0], 1).tolist()\n",
    "    train_indices = random.sample(indices, n_train)\n",
    "    train_dataset = dataset.iloc[train_indices, :]\n",
    "\n",
    "    # apply scaling\n",
    "    if scale_data:\n",
    "        standard_scaler = StandardScaler()\n",
    "        predictors = dataset.columns.tolist()\n",
    "        predictors.remove(\"quality_label\")\n",
    "        train_dataset[predictors] = standard_scaler.fit_transform(\n",
    "            train_dataset[predictors]\n",
    "        )\n",
    "\n",
    "    # train model on this fold\n",
    "    try:\n",
    "        model_1 = sm.GLM.from_formula(\n",
    "            model_formula,\n",
    "            family=sm.families.Binomial(),\n",
    "            data=train_dataset,\n",
    "        )\n",
    "        result_1 = model_1.fit()\n",
    "    except:\n",
    "        raise ValueError(\"Model formula is wrong\")\n",
    "\n",
    "    # TEST -----------\n",
    "    # create test dataset with remaining instances\n",
    "    # make sure to drop quality label from test dataset\n",
    "    test_indices = list(set(indices) - set(train_indices))\n",
    "    test_dataset = dataset.iloc[test_indices, :]\n",
    "    test_label = test_dataset[\"quality_label\"]\n",
    "    test_dataset = test_dataset.drop(columns=[\"quality_label\"])\n",
    "\n",
    "    # apply scaling\n",
    "    if scale_data:\n",
    "        test_dataset[predictors] = standard_scaler.fit_transform(\n",
    "            test_dataset[predictors]\n",
    "        )\n",
    "\n",
    "    # unit-test\n",
    "    assert not \"quality_label\" in test_dataset.columns, \"drop quality label from test\"\n",
    "\n",
    "    # reorder test dataset features and add intercept\n",
    "    # to make predictions\n",
    "    features = result_1.params.index[1:]\n",
    "    test_features = test_dataset.loc[:, features]\n",
    "    test_features.insert(0, \"intercept\", 1)\n",
    "\n",
    "    # unit-test\n",
    "    assert not \"quality_label\" in features, \"drop quality label from features\"\n",
    "\n",
    "    # predict -------------\n",
    "    # thresholded binary predictions\n",
    "    predictions = (result_1.predict(test_features) >= thresh).astype(int)\n",
    "    precision = metrics.precision_score(test_label, predictions)\n",
    "    recall = metrics.recall_score(test_label, predictions)\n",
    "    return {\"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "def train_classifier_on_full_dataset(\n",
    "    model_formula: str,\n",
    "    dataset: pd.DataFrame,\n",
    "):\n",
    "    \"\"\"train the classifier on the full dataset\n",
    "\n",
    "    Args:\n",
    "        model_formula\n",
    "\n",
    "    Returns:\n",
    "        precisions and recalls for each fold\n",
    "    \"\"\"\n",
    "    # unit-test\n",
    "    # this is a ground truth label that should not\n",
    "    # be in the dataset\n",
    "    assert (\n",
    "        not \"sorting_accuracy\" in dataset.columns\n",
    "    ), \"drop sorting_accuracy from dataset\"\n",
    "\n",
    "    # train the model\n",
    "    try:\n",
    "        model = sm.GLM.from_formula(\n",
    "            model_formula,\n",
    "            family=sm.families.Binomial(),\n",
    "            data=dataset,\n",
    "        )\n",
    "        result = model.fit()\n",
    "    except:\n",
    "        raise ValueError(\"Model formula is wrong\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_crossval_flclassifier_metrics(\n",
    "    dataset: pd.DataFrame,\n",
    "    model_formula: str,\n",
    "    split_ratio: float = 0.75,\n",
    "    seeds: np.array = np.arange(0, 100, 1),\n",
    "    thresh: float = 0.8,\n",
    "    scale_data=False,\n",
    "):\n",
    "    \"\"\"Calculate cross-validated mcfadden pseudo r-squared\n",
    "    on test dataset\n",
    "\n",
    "    Args:\n",
    "        model_formula (str): glm model formula\n",
    "        split_ratio (float, optional): _description_. Defaults to 0.75.\n",
    "        seeds (np.array, optional): _description_. Defaults to np.arange(0, 100, 1).\n",
    "\n",
    "    Returns:\n",
    "        np.array: mcfadden pseudo r-squared\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for seed in range(len(seeds)):\n",
    "        result = get_single_fold_flclassifier_metrics(\n",
    "            model_formula,\n",
    "            dataset,\n",
    "            split_ratio=split_ratio,\n",
    "            seed=seed,\n",
    "            thresh=thresh,\n",
    "            scale_data=scale_data,\n",
    "        )\n",
    "        results.append(result)\n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "def train_test_eval(model_formula, dataset, seeds, scale_data=False):\n",
    "\n",
    "    # create model\n",
    "    # model = create_classifier_model(predictive_metrics)\n",
    "    # print(model)\n",
    "\n",
    "    # evaluate R2\n",
    "    rez_metrics = get_crossval_flclassifier_metrics(\n",
    "        dataset=dataset,\n",
    "        model_formula=model_formula,\n",
    "        split_ratio=0.75,\n",
    "        seeds=seeds,\n",
    "        thresh=0.8,\n",
    "        scale_data=scale_data,\n",
    "    )\n",
    "\n",
    "    # PERFORMANCE -----------------------------\n",
    "\n",
    "    # make arrays\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for m_i in rez_metrics:\n",
    "        precisions.append(m_i[\"precision\"])\n",
    "        recalls.append(m_i[\"recall\"])\n",
    "\n",
    "    # precision\n",
    "    print(\"\\nprecision:\")\n",
    "    print(\"median:\", np.nanmedian(precisions))\n",
    "    print(\"std:\", np.nanstd(precisions))\n",
    "    print(\"95% CI:\", 1.96 * np.std(precisions) / np.sqrt(len(precisions)))\n",
    "\n",
    "    # recall\n",
    "    print(\"\\nrecall:\")\n",
    "    print(\"median r2:\", np.nanmedian(recalls))\n",
    "    print(\"std r2:\", np.nanstd(recalls))\n",
    "    print(\"95% CI:\", 1.96 * np.std(recalls) / np.sqrt(len(recalls)))\n",
    "\n",
    "    # plot performance\n",
    "    _, ax = plt.subplots(figsize=(0.7, 1))\n",
    "    df = pd.DataFrame(data=[precisions, recalls], index=[\"precision\", \"recall\"]).T\n",
    "    sns.stripplot(ax=ax, data=df, jitter=0.04, color=\"k\", size=3)\n",
    "\n",
    "    # stats precision\n",
    "    ax.errorbar(\n",
    "        x=0,\n",
    "        y=np.nanmedian(precisions),\n",
    "        yerr=1.96 * np.std(precisions) / np.sqrt(len(precisions)),  # 95% ci\n",
    "        marker=\"o\",\n",
    "        color=\"orange\",\n",
    "        markeredgecolor=\"w\",\n",
    "        markersize=5,\n",
    "        zorder=np.inf,\n",
    "    )\n",
    "\n",
    "    # stats recall\n",
    "    ax.errorbar(\n",
    "        x=1,\n",
    "        y=np.nanmedian(recalls),\n",
    "        yerr=1.96 * np.std(recalls) / np.sqrt(len(recalls)),  # 95% ci\n",
    "        marker=\"o\",\n",
    "        color=\"orange\",\n",
    "        markeredgecolor=\"w\",\n",
    "        markersize=5,\n",
    "        zorder=np.inf,\n",
    "    )\n",
    "\n",
    "    # disconnect axes (R style)\n",
    "    ax.spines[[\"right\", \"top\"]].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_position((\"axes\", -0.05))\n",
    "    ax.yaxis.set_ticks_position(\"left\")\n",
    "    ax.spines[\"left\"].set_position((\"axes\", -0.05))\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    # labels\n",
    "    ax.set_ylim([0, 1])\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "def interpret_features_weights(model_formula, dataset, predictive_metrics):\n",
    "\n",
    "    model_result = train_classifier_on_full_dataset(\n",
    "        model_formula=model_formula, dataset=dataset\n",
    "    )\n",
    "    weights = model_result.params\n",
    "\n",
    "    # build table of features contributions  ---------------\n",
    "\n",
    "    # add feature weights\n",
    "    weights_for_df = weights.drop(index=\"Intercept\")\n",
    "    weights_df = weights_for_df.to_frame()\n",
    "    weights_df.columns = [\"weights\"]\n",
    "\n",
    "    # add p-values (fit to entire dataset)\n",
    "    pvalues_df = model_result.pvalues.drop(index=\"Intercept\")\n",
    "    pvalues_df = pvalues_df.to_frame()\n",
    "    pvalues_df.columns = [\"weight p-value\"]\n",
    "    data_df = pd.merge(\n",
    "        weights_df,\n",
    "        pvalues_df,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # interpret interceptsl\n",
    "    print(\"Intercept:\")\n",
    "    print(\"- logodds of detected HQ units:\", weights[\"Intercept\"])\n",
    "    odds = np.exp(weights[\"Intercept\"])\n",
    "    print(\n",
    "        \"- P(Identified HQ units when all features are null):\",\n",
    "        odds / (1 + odds),\n",
    "    )\n",
    "\n",
    "    # interpret all features\n",
    "    odds = np.exp(weights[\"Intercept\"] + data_df[\"weights\"])\n",
    "    data_df[r\"\\Delta accuracy\"] = np.sign(data_df[\"weights\"]) * odds / (1 + odds)\n",
    "\n",
    "    # display\n",
    "    data_df = data_df.sort_values(by=[r\"\\Delta accuracy\"], ascending=False)\n",
    "    display(data_df)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "* the dataset is a dataframe that contains the sorted single-units (indices), their quality metrics and their quality label ()\"good\" or \"bad\" units evaluated with our ground truth, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steeve/steeve/epfl/code/spikebias/envs/spikebias/lib/python3.9/site-packages/spikeinterface/core/base.py:1079: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommonReferenceRecording: 384 channels - 40.0kHz - 1 segments - 82,319,958 samples \n",
       "                          2,058.00s (34.30 minutes) - float32 dtype - 117.76 GiB"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steeve/steeve/epfl/code/spikebias/envs/spikebias/lib/python3.9/site-packages/spikeinterface/core/base.py:1079: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.103.0 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NumpyFolderSorting: 1388 units - 1 segments - 40.0kHz"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "FrameSliceSorting: 520 units - 1 segments - 40.0kHz"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# waveform extraction parameters\n",
    "FREQ_MIN = 300\n",
    "REFERENCE = \"global\"\n",
    "OPERATOR = \"median\"\n",
    "MS_BEFORE = 6 # ms\n",
    "ms_after = 6\n",
    "LOAD_WE_IF_EXISTS = True\n",
    "\n",
    "# load recording\n",
    "RecordingNS = si.load_extractor(RECORDING_NS)\n",
    "RecordingNS.set_property('location', RecordingNS.get_property('location')[:,:2])\n",
    "\n",
    "# preprocess\n",
    "RecordingNS = spre.highpass_filter(RecordingNS, freq_min=FREQ_MIN)\n",
    "RecordingNS = spre.common_reference(RecordingNS, reference=REFERENCE, operator=OPERATOR)\n",
    "display(RecordingNS)\n",
    "\n",
    "# load ground truth \n",
    "GroundTruth = si.load_extractor(GT_ns)\n",
    "GroundTruth.frame_slice(start_frame=0, end_frame=RecordingNS.get_sampling_frequency() * DURATION_SEC)\n",
    "GroundTruth.save(folder=GT_ns_10m, overwrite=True)\n",
    "display(GroundTruth)\n",
    "\n",
    "# load sorting \n",
    "SortingNS = si.load_extractor(KS4_ns_10m)\n",
    "SortingNS = remove_excess_spikes(SortingNS, RecordingNS)\n",
    "SortingNS = SortingNS.frame_slice(start_frame=0, end_frame=RecordingNS.get_sampling_frequency() * DURATION_SEC)\n",
    "display(SortingNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute/save waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b56c260a2594325aa27d77f81cdbc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fitting PCA:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d580f1adce044cda98902720f2b1be74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Projecting waveforms:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "WaveformExtractor: 384 channels - 184 units - 1 segments\n",
       "  before:240 after:240 n_per_units:500 - sparse"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 4min 12s, total: 5min 28s\n",
      "Wall time: 6min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# extract/save sorted unit waveforms\n",
    "WeNS = extract_waveforms(RecordingNS, SortingNS, STUDY_ns, mode=\"folder\", sparse=True, ms_before=MS_BEFORE,\n",
    "                    ms_after=ms_after, max_spikes_per_unit=MAX_SPIKES_PER_UNIT,\n",
    "                    unit_batch_size=CHUNKS, overwrite=True,\n",
    "                    seed=0, **WE_JOB_KWARGS)\n",
    "display(\"Waveform extractor for sorted units:\", WeNS)\n",
    "\n",
    "\n",
    "# extract/save single-unit waveforms\n",
    "WeNS_su = get_waveformExtractor_for_single_units(\n",
    "    KS4_ns_10m, STUDY_ns, STUDY_ns_su, n_sites=384, load_if_exists=False\n",
    ")\n",
    "display(\"Waveform extractor for single units:\", WeNS_su)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9e7b1c53994db39b6b7a9c1693e31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fitting PCA:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121806527a894440b97220812fea6c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Projecting waveforms:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e005b0db0fc4cac9ef988fda58e7efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "extract amplitudes:   0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steeve/steeve/epfl/code/spikebias/envs/spikebias/lib/python3.9/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/steeve/steeve/epfl/code/spikebias/envs/spikebias/lib/python3.9/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/steeve/steeve/epfl/code/spikebias/envs/spikebias/lib/python3.9/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/steeve/steeve/epfl/code/spikebias/envs/spikebias/lib/python3.9/site-packages/spikeinterface/qualitymetrics/quality_metric_calculator.py:132: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  metrics.loc[non_empty_unit_ids, col] = pd.Series(res[i])\n",
      "/home/steeve/steeve/epfl/code/spikebias/envs/spikebias/lib/python3.9/site-packages/spikeinterface/qualitymetrics/quality_metric_calculator.py:127: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  metrics.loc[non_empty_unit_ids, metric_name] = pd.Series(res)\n",
      "/home/steeve/steeve/epfl/code/spikebias/envs/spikebias/lib/python3.9/site-packages/spikeinterface/qualitymetrics/quality_metric_calculator.py:132: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  metrics.loc[non_empty_unit_ids, col] = pd.Series(res[i])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cfb32c91d641538d99ffa09fe06bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************** Analysing data completion ***************\n",
      "Data completion: amplitude_cutoff        184\n",
      "firing_range            184\n",
      "firing_rate             184\n",
      "isi_violations_ratio    184\n",
      "presence_ratio          184\n",
      "rp_contamination        184\n",
      "rp_violations           184\n",
      "sd_ratio                184\n",
      "snr                     184\n",
      "silhouette              182\n",
      "dtype: int64\n",
      "quality metrics are: Index(['amplitude_cutoff', 'firing_range', 'firing_rate',\n",
      "       'isi_violations_ratio', 'presence_ratio', 'rp_contamination',\n",
      "       'rp_violations', 'sd_ratio', 'snr', 'silhouette'],\n",
      "      dtype='object')\n",
      "2025-10-09 15:36:18,033 - root - dataloader.py - load_dataset - INFO - CURATION ----------------------------\n",
      "2025-10-09 15:36:18,036 - root - dataloader.py - load_dataset - INFO - nb of units before filtering units with inf feature: 184\n",
      "2025-10-09 15:36:18,036 - root - dataloader.py - load_dataset - INFO - nb of units after: 184\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 110x110 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create the dataset\n",
    "data = dataloader.load_dataset(\n",
    "    quality_path, \"NS\", \"KS4\", KS4_ns_10m, STUDY_ns, STUDY_ns_su, GT_ns_10m\n",
    ")\n",
    "\n",
    "# instantiate the model\n",
    "flcmodel = FlcModel(data[\"predictors\"])\n",
    "\n",
    "# train & eval. w/ cross-val.\n",
    "results = flcmodel.evaluate(data[\"dataset\"], **model_prms)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.1, 1.1))\n",
    "ax = flcplot.plot(ax, results[\"metric_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amplitude_cutoff</th>\n",
       "      <th>firing_range</th>\n",
       "      <th>firing_rate</th>\n",
       "      <th>isi_violations_ratio</th>\n",
       "      <th>rp_contamination</th>\n",
       "      <th>rp_violations</th>\n",
       "      <th>sd_ratio</th>\n",
       "      <th>snr</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>mad_ratio</th>\n",
       "      <th>quality_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>amplitude_cutoff</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.156329</td>\n",
       "      <td>-0.154962</td>\n",
       "      <td>0.294967</td>\n",
       "      <td>0.062296</td>\n",
       "      <td>-0.031972</td>\n",
       "      <td>0.425050</td>\n",
       "      <td>0.599958</td>\n",
       "      <td>0.390668</td>\n",
       "      <td>0.246609</td>\n",
       "      <td>-0.102261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firing_range</th>\n",
       "      <td>-0.156329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996525</td>\n",
       "      <td>-0.081202</td>\n",
       "      <td>0.206018</td>\n",
       "      <td>0.442963</td>\n",
       "      <td>-0.092564</td>\n",
       "      <td>-0.215906</td>\n",
       "      <td>-0.268055</td>\n",
       "      <td>0.020799</td>\n",
       "      <td>0.026902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firing_rate</th>\n",
       "      <td>-0.154962</td>\n",
       "      <td>0.996525</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.080729</td>\n",
       "      <td>0.198882</td>\n",
       "      <td>0.429786</td>\n",
       "      <td>-0.105102</td>\n",
       "      <td>-0.208145</td>\n",
       "      <td>-0.246225</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isi_violations_ratio</th>\n",
       "      <td>0.294967</td>\n",
       "      <td>-0.081202</td>\n",
       "      <td>-0.080729</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.436161</td>\n",
       "      <td>0.182912</td>\n",
       "      <td>0.216974</td>\n",
       "      <td>0.288456</td>\n",
       "      <td>0.169565</td>\n",
       "      <td>0.064628</td>\n",
       "      <td>0.023906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rp_contamination</th>\n",
       "      <td>0.062296</td>\n",
       "      <td>0.206018</td>\n",
       "      <td>0.198882</td>\n",
       "      <td>0.436161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555389</td>\n",
       "      <td>0.072141</td>\n",
       "      <td>-0.032071</td>\n",
       "      <td>-0.040623</td>\n",
       "      <td>0.074808</td>\n",
       "      <td>-0.015572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rp_violations</th>\n",
       "      <td>-0.031972</td>\n",
       "      <td>0.442963</td>\n",
       "      <td>0.429786</td>\n",
       "      <td>0.182912</td>\n",
       "      <td>0.555389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001898</td>\n",
       "      <td>-0.077760</td>\n",
       "      <td>-0.107252</td>\n",
       "      <td>0.011088</td>\n",
       "      <td>-0.043278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd_ratio</th>\n",
       "      <td>0.425050</td>\n",
       "      <td>-0.092564</td>\n",
       "      <td>-0.105102</td>\n",
       "      <td>0.216974</td>\n",
       "      <td>0.072141</td>\n",
       "      <td>-0.001898</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.409099</td>\n",
       "      <td>0.054188</td>\n",
       "      <td>0.879606</td>\n",
       "      <td>-0.256566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snr</th>\n",
       "      <td>0.599958</td>\n",
       "      <td>-0.215906</td>\n",
       "      <td>-0.208145</td>\n",
       "      <td>0.288456</td>\n",
       "      <td>-0.032071</td>\n",
       "      <td>-0.077760</td>\n",
       "      <td>0.409099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.563776</td>\n",
       "      <td>0.095749</td>\n",
       "      <td>0.216270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>silhouette</th>\n",
       "      <td>0.390668</td>\n",
       "      <td>-0.268055</td>\n",
       "      <td>-0.246225</td>\n",
       "      <td>0.169565</td>\n",
       "      <td>-0.040623</td>\n",
       "      <td>-0.107252</td>\n",
       "      <td>0.054188</td>\n",
       "      <td>0.563776</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.142673</td>\n",
       "      <td>0.147933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mad_ratio</th>\n",
       "      <td>0.246609</td>\n",
       "      <td>0.020799</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.064628</td>\n",
       "      <td>0.074808</td>\n",
       "      <td>0.011088</td>\n",
       "      <td>0.879606</td>\n",
       "      <td>0.095749</td>\n",
       "      <td>-0.142673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.423541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_label</th>\n",
       "      <td>-0.102261</td>\n",
       "      <td>0.026902</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.023906</td>\n",
       "      <td>-0.015572</td>\n",
       "      <td>-0.043278</td>\n",
       "      <td>-0.256566</td>\n",
       "      <td>0.216270</td>\n",
       "      <td>0.147933</td>\n",
       "      <td>-0.423541</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      amplitude_cutoff  firing_range  firing_rate  \\\n",
       "amplitude_cutoff              1.000000     -0.156329    -0.154962   \n",
       "firing_range                 -0.156329      1.000000     0.996525   \n",
       "firing_rate                  -0.154962      0.996525     1.000000   \n",
       "isi_violations_ratio          0.294967     -0.081202    -0.080729   \n",
       "rp_contamination              0.062296      0.206018     0.198882   \n",
       "rp_violations                -0.031972      0.442963     0.429786   \n",
       "sd_ratio                      0.425050     -0.092564    -0.105102   \n",
       "snr                           0.599958     -0.215906    -0.208145   \n",
       "silhouette                    0.390668     -0.268055    -0.246225   \n",
       "mad_ratio                     0.246609      0.020799     0.001598   \n",
       "quality_label                -0.102261      0.026902     0.066000   \n",
       "\n",
       "                      isi_violations_ratio  rp_contamination  rp_violations  \\\n",
       "amplitude_cutoff                  0.294967          0.062296      -0.031972   \n",
       "firing_range                     -0.081202          0.206018       0.442963   \n",
       "firing_rate                      -0.080729          0.198882       0.429786   \n",
       "isi_violations_ratio              1.000000          0.436161       0.182912   \n",
       "rp_contamination                  0.436161          1.000000       0.555389   \n",
       "rp_violations                     0.182912          0.555389       1.000000   \n",
       "sd_ratio                          0.216974          0.072141      -0.001898   \n",
       "snr                               0.288456         -0.032071      -0.077760   \n",
       "silhouette                        0.169565         -0.040623      -0.107252   \n",
       "mad_ratio                         0.064628          0.074808       0.011088   \n",
       "quality_label                     0.023906         -0.015572      -0.043278   \n",
       "\n",
       "                      sd_ratio       snr  silhouette  mad_ratio  quality_label  \n",
       "amplitude_cutoff      0.425050  0.599958    0.390668   0.246609      -0.102261  \n",
       "firing_range         -0.092564 -0.215906   -0.268055   0.020799       0.026902  \n",
       "firing_rate          -0.105102 -0.208145   -0.246225   0.001598       0.066000  \n",
       "isi_violations_ratio  0.216974  0.288456    0.169565   0.064628       0.023906  \n",
       "rp_contamination      0.072141 -0.032071   -0.040623   0.074808      -0.015572  \n",
       "rp_violations        -0.001898 -0.077760   -0.107252   0.011088      -0.043278  \n",
       "sd_ratio              1.000000  0.409099    0.054188   0.879606      -0.256566  \n",
       "snr                   0.409099  1.000000    0.563776   0.095749       0.216270  \n",
       "silhouette            0.054188  0.563776    1.000000  -0.142673       0.147933  \n",
       "mad_ratio             0.879606  0.095749   -0.142673   1.000000      -0.423541  \n",
       "quality_label        -0.256566  0.216270    0.147933  -0.423541       1.000000  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select uncorrelated metrics\n",
    "data[\"dataset\"].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10s)Train & eval classifier model\n",
    "\n",
    "* z-scoring the features marginally changed the results. So we used the raw data, to produce more interpretable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "precision:\n",
      "median: 0.8823529411764706\n",
      "std: 0.08928325003307916\n",
      "95% CI: 0.017499517006483514\n",
      "\n",
      "recall:\n",
      "median r2: 0.6220238095238095\n",
      "std r2: 0.08884789760541194\n",
      "95% CI: 0.01741418793066074\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 70x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "seeds = np.arange(0, 100, 1)\n",
    "\n",
    "# curate dataset\n",
    "dataset_c = copy.copy(data[\"dataset\"])\n",
    "\n",
    "# (10s) evaluate\n",
    "precisions, recalls = train_test_eval(flcmodel.formula, dataset_c, seeds, scale_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature weights\n",
    "\n",
    "* obtained from fitting the same model to the entire dataset, with z-scored feature. \n",
    "* note: an increase of mad_ratio by 0.1 (scale is 1 in the table) increases p(\"good\") by 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amplitude_cutoff</th>\n",
       "      <th>firing_range</th>\n",
       "      <th>firing_rate</th>\n",
       "      <th>isi_violations_ratio</th>\n",
       "      <th>rp_contamination</th>\n",
       "      <th>rp_violations</th>\n",
       "      <th>sd_ratio</th>\n",
       "      <th>snr</th>\n",
       "      <th>silhouette</th>\n",
       "      <th>mad_ratio</th>\n",
       "      <th>quality_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.282089</td>\n",
       "      <td>0.426021</td>\n",
       "      <td>0.487198</td>\n",
       "      <td>-0.157828</td>\n",
       "      <td>-0.398486</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>3.398728</td>\n",
       "      <td>-0.439852</td>\n",
       "      <td>-0.946623</td>\n",
       "      <td>1.876675</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-0.379877</td>\n",
       "      <td>-0.357146</td>\n",
       "      <td>-0.356425</td>\n",
       "      <td>-0.157828</td>\n",
       "      <td>-0.398486</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>-0.425340</td>\n",
       "      <td>0.717828</td>\n",
       "      <td>0.837074</td>\n",
       "      <td>-0.656819</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1.398181</td>\n",
       "      <td>0.212430</td>\n",
       "      <td>0.232852</td>\n",
       "      <td>-0.157828</td>\n",
       "      <td>-0.398486</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>1.022098</td>\n",
       "      <td>1.987576</td>\n",
       "      <td>0.742010</td>\n",
       "      <td>0.120169</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>-0.355970</td>\n",
       "      <td>0.354824</td>\n",
       "      <td>0.360025</td>\n",
       "      <td>-0.157828</td>\n",
       "      <td>-0.398486</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>-0.503933</td>\n",
       "      <td>1.098097</td>\n",
       "      <td>1.116058</td>\n",
       "      <td>-0.743439</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>-0.329948</td>\n",
       "      <td>-0.214752</td>\n",
       "      <td>-0.152445</td>\n",
       "      <td>-0.157828</td>\n",
       "      <td>-0.398486</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>-0.494971</td>\n",
       "      <td>-0.560571</td>\n",
       "      <td>-0.541591</td>\n",
       "      <td>-0.471985</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>-0.359753</td>\n",
       "      <td>-0.641934</td>\n",
       "      <td>-0.622104</td>\n",
       "      <td>-0.157828</td>\n",
       "      <td>-0.398486</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>-0.412456</td>\n",
       "      <td>-0.544053</td>\n",
       "      <td>-0.825574</td>\n",
       "      <td>-0.209978</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>-0.270368</td>\n",
       "      <td>0.354824</td>\n",
       "      <td>0.344915</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>2.868329</td>\n",
       "      <td>0.378245</td>\n",
       "      <td>1.077628</td>\n",
       "      <td>-0.430864</td>\n",
       "      <td>-0.946499</td>\n",
       "      <td>2.083861</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>-0.332284</td>\n",
       "      <td>-0.214752</td>\n",
       "      <td>-0.294728</td>\n",
       "      <td>-0.157828</td>\n",
       "      <td>-0.398486</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>-0.428799</td>\n",
       "      <td>-0.598953</td>\n",
       "      <td>-0.994342</td>\n",
       "      <td>-0.333232</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>-0.375433</td>\n",
       "      <td>-0.641934</td>\n",
       "      <td>-0.648546</td>\n",
       "      <td>-0.157828</td>\n",
       "      <td>-0.398486</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>-0.465901</td>\n",
       "      <td>-0.746767</td>\n",
       "      <td>-0.206396</td>\n",
       "      <td>-0.397962</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>-0.250800</td>\n",
       "      <td>-0.641934</td>\n",
       "      <td>-0.613290</td>\n",
       "      <td>-0.157828</td>\n",
       "      <td>-0.398486</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>0.224411</td>\n",
       "      <td>-0.390656</td>\n",
       "      <td>-0.730153</td>\n",
       "      <td>0.342985</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     amplitude_cutoff  firing_range  firing_rate  isi_violations_ratio  \\\n",
       "198         -0.282089      0.426021     0.487198             -0.157828   \n",
       "199         -0.379877     -0.357146    -0.356425             -0.157828   \n",
       "213          1.398181      0.212430     0.232852             -0.157828   \n",
       "214         -0.355970      0.354824     0.360025             -0.157828   \n",
       "231         -0.329948     -0.214752    -0.152445             -0.157828   \n",
       "..                ...           ...          ...                   ...   \n",
       "396         -0.359753     -0.641934    -0.622104             -0.157828   \n",
       "398         -0.270368      0.354824     0.344915              0.000872   \n",
       "400         -0.332284     -0.214752    -0.294728             -0.157828   \n",
       "405         -0.375433     -0.641934    -0.648546             -0.157828   \n",
       "371         -0.250800     -0.641934    -0.613290             -0.157828   \n",
       "\n",
       "     rp_contamination  rp_violations  sd_ratio       snr  silhouette  \\\n",
       "198         -0.398486      -0.248756  3.398728 -0.439852   -0.946623   \n",
       "199         -0.398486      -0.248756 -0.425340  0.717828    0.837074   \n",
       "213         -0.398486      -0.248756  1.022098  1.987576    0.742010   \n",
       "214         -0.398486      -0.248756 -0.503933  1.098097    1.116058   \n",
       "231         -0.398486      -0.248756 -0.494971 -0.560571   -0.541591   \n",
       "..                ...            ...       ...       ...         ...   \n",
       "396         -0.398486      -0.248756 -0.412456 -0.544053   -0.825574   \n",
       "398          2.868329       0.378245  1.077628 -0.430864   -0.946499   \n",
       "400         -0.398486      -0.248756 -0.428799 -0.598953   -0.994342   \n",
       "405         -0.398486      -0.248756 -0.465901 -0.746767   -0.206396   \n",
       "371         -0.398486      -0.248756  0.224411 -0.390656   -0.730153   \n",
       "\n",
       "     mad_ratio  quality_label  \n",
       "198   1.876675            1.0  \n",
       "199  -0.656819            1.0  \n",
       "213   0.120169            1.0  \n",
       "214  -0.743439            1.0  \n",
       "231  -0.471985            1.0  \n",
       "..         ...            ...  \n",
       "396  -0.209978            0.0  \n",
       "398   2.083861            0.0  \n",
       "400  -0.333232            0.0  \n",
       "405  -0.397962            0.0  \n",
       "371   0.342985            0.0  \n",
       "\n",
       "[184 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:\n",
      "- logodds of detected HQ units: -2.6460174357603847\n",
      "- P(Identified HQ units when all features are null): 0.06623489737447741\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "      <th>weight p-value</th>\n",
       "      <th>\\Delta accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>firing_rate</th>\n",
       "      <td>25.236184</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd_ratio</th>\n",
       "      <td>7.344015</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>9.909688e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rp_contamination</th>\n",
       "      <td>0.144078</td>\n",
       "      <td>0.756604</td>\n",
       "      <td>7.572236e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isi_violations_ratio</th>\n",
       "      <td>0.126328</td>\n",
       "      <td>0.767663</td>\n",
       "      <td>7.448932e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firing_range</th>\n",
       "      <td>-24.788286</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-1.217400e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mad_ratio</th>\n",
       "      <td>-12.773626</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-2.010639e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amplitude_cutoff</th>\n",
       "      <td>-1.395003</td>\n",
       "      <td>0.040850</td>\n",
       "      <td>-1.727582e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>silhouette</th>\n",
       "      <td>-0.477710</td>\n",
       "      <td>0.105116</td>\n",
       "      <td>-4.213906e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rp_violations</th>\n",
       "      <td>-0.400855</td>\n",
       "      <td>0.526909</td>\n",
       "      <td>-4.535267e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snr</th>\n",
       "      <td>-0.174073</td>\n",
       "      <td>0.767484</td>\n",
       "      <td>-5.624811e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        weights  weight p-value  \\Delta accuracy\n",
       "firing_rate           25.236184        0.000005     1.000000e+00\n",
       "sd_ratio               7.344015        0.000085     9.909688e-01\n",
       "rp_contamination       0.144078        0.756604     7.572236e-02\n",
       "isi_violations_ratio   0.126328        0.767663     7.448932e-02\n",
       "firing_range         -24.788286        0.000006    -1.217400e-12\n",
       "mad_ratio            -12.773626        0.000011    -2.010639e-07\n",
       "amplitude_cutoff      -1.395003        0.040850    -1.727582e-02\n",
       "silhouette            -0.477710        0.105116    -4.213906e-02\n",
       "rp_violations         -0.400855        0.526909    -4.535267e-02\n",
       "snr                   -0.174073        0.767484    -5.624811e-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# z-score the features\n",
    "standard_scaler = StandardScaler()\n",
    "predictors = dataset_c.columns.tolist()\n",
    "predictors.remove(\"quality_label\")\n",
    "dataset_c[predictors] = standard_scaler.fit_transform(dataset_c[predictors])\n",
    "display(dataset_c)\n",
    "\n",
    "# compute feature weights on entire dataset\n",
    "table_data = interpret_features_weights(flcmodel.formula, dataset_c, data[\"predictors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.5, 1.5))\n",
    "\n",
    "# plot\n",
    "ax = table_data.sort_values(by=\"weights\").plot.barh(\n",
    "    ax=ax, y=\"weights\", color=\"k\", width=0.8\n",
    ")\n",
    "\n",
    "# aesthetics\n",
    "ax.spines[[\"right\", \"top\"]].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_position((\"axes\", -0.05))\n",
    "ax.spines[\"left\"].set_position((\"axes\", -0.05))\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# no legend\n",
    "ax.legend().set_visible(False)\n",
    "ax.set_xlabel(\"Unit feature weights\")\n",
    "\n",
    "plt.savefig(\"figures/1_curation/fig5_G_weights.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (190s)Shuffled control\n",
    "\n",
    "* TODO: repeat shuffling for many random seeds. I expect 50% median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameters\n",
    "# np.random.seed(0)\n",
    "\n",
    "# # shuffle labels\n",
    "# dataset_shuff = copy.copy(dataset_c)\n",
    "# dataset_shuff[\"quality_label\"] = np.random.permutation(dataset_c[\"quality_label\"])\n",
    "\n",
    "# # train\n",
    "# precisions, recalls = train_test_eval(dataset_shuff, data[\"predictors\"], seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight of shuffled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_data = interpret_features_weights(model, dataset_shuff, predictive_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(GT_ns_10m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spikebias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
