{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros of added details\n",
    "\n",
    "author: laquitainesteeve@gmail.com\n",
    "\n",
    "Execution time:\n",
    "\n",
    "Tested on Ubuntu 24.04.1 LTS (32 cores, 188 GB RAM, Intel(R) Core(TM) i9-14900K ï¼ 3.2 GHz/5.8 GHz) with RTX 5090 GPU with 40GB VRAM\n",
    "\n",
    "prerequisites:\n",
    "- 15 MB to store cell models\n",
    "- sorted recordings with kilosort4  \n",
    "    - Buccino [DONE]\n",
    "    - Buccino clone [DONE]\n",
    "\n",
    "Note:\n",
    "- the original Buccino dataset has no metadata about the cells and we have no control on the parameters so we used MEAREC to re-simulate it, then we'll modify one parameter at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "Activate mearec virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import packages\n",
    "import MEArec as mr\n",
    "import os \n",
    "import numpy as np\n",
    "import spikeinterface.extractors as se \n",
    "import spikeinterface.sorters as sorters \n",
    "import spikeinterface.comparison as sc  \n",
    "from matplotlib import pyplot as plt;\n",
    "import spikeinterface as si\n",
    "from spikeinterface import extract_waveforms\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import spikeinterface.core.template_tools as ttools\n",
    "from numpy.linalg import norm as lalgnorm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SET PROJECT PATH\n",
    "\n",
    "PROJ_PATH = \"/home/steeve/steeve/epfl/code/spikebias/\"\n",
    "os.chdir(PROJ_PATH)\n",
    "\n",
    "from src.nodes.validation import firing_rate as fr\n",
    "from src.nodes.analysis.failures import isolation\n",
    "\n",
    "# parameters\n",
    "DURATION_S = 600 # duration of recording period considered in seconds\n",
    "\n",
    "# SET DATA PATHS\n",
    "\n",
    "# original buccino simulation\n",
    "RECORDING_BUCCINO_PATH = 'dataset/01_intermediate/preprocessing/recording_buccino/'\n",
    "WAVEFORM_GT_BUCCINO_PATH =  './temp/pros_of_details/waveformextractor_buccino/'\n",
    "\n",
    "# setup simulation common template parameters\n",
    "DATA_PATH = './temp/pros_of_details/'\n",
    "CELLS_PATH = mr.get_default_cell_models_folder()\n",
    "TEMPLATES_PATH = './temp/pros_of_details/templates.h5'\n",
    "\n",
    "# setup synthetic simulation version 2 common parameters\n",
    "RECORDING_CLONE_H5_PATH = './temp/pros_of_details/recordings.h5'\n",
    "RECORDING_CLONE_PATH = 'dataset/00_raw/recording_buccino_clone/'\n",
    "GROUND_TRUTH_CLONE_PATH = 'dataset/00_raw/ground_truth_buccino_clone/'\n",
    "WAVEFORM_GT_CLONE_PATH =  './temp/pros_of_details/waveformextractor_buccino_clone/'\n",
    "\n",
    "# setup synthetic synthetic version 2 parameters\n",
    "RECORDING_S2_H5_PATH = './temp/pros_of_details/recordings_s2.h5'\n",
    "RECORDING_S2_PATH = 'dataset/00_raw/recording_buccino_s2/'\n",
    "GROUND_TRUTH_S2_PATH = 'dataset/00_raw/ground_truth_buccino_s2/'\n",
    "WAVEFORM_GT_S2_PATH =  './temp/pros_of_details/waveformextractor_buccino_s2/'\n",
    "\n",
    "# parallel processing parameters\n",
    "N_JOBS = 20\n",
    "CHUNKS = 50000\n",
    "\n",
    "# npx_spont\n",
    "RECORDING_NS_PATH = 'dataset/01_intermediate/preprocessing/recording_npx_spont/'\n",
    "GROUND_TRUTH_NS_PATH = 'dataset/00_raw/ground_truth_npx_spont/'\n",
    "WAVEFORM_GT_NS_PATH = 'dataset/01_intermediate/waveforms/ground_truth_npx_spont/'\n",
    "\n",
    "# parallel processing parameters \n",
    "job_kwargs = dict(n_jobs=-1, chunk_duration=\"1s\", progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS \n",
    "\n",
    "def _euclidean_distance(coord_1, coord_2):\n",
    "    return np.sqrt(np.sum((coord_1 - coord_2) ** 2))\n",
    "\n",
    "\n",
    "def mad(data):\n",
    "    mean_data = np.mean(data)\n",
    "    return np.mean(np.absolute(data - mean_data))\n",
    "\n",
    "\n",
    "def get_mad_ratio(spike_amp, noise_amp):\n",
    "    \"\"\"calculate an sd_ratio robust to outliers\n",
    "\n",
    "    Args:\n",
    "        spike_amp (_type_): _description_\n",
    "        noise_amp (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    mad_unit = mad(spike_amp)  # twice smaller than std\n",
    "    mad_noise = mad(noise_amp)\n",
    "    return mad_unit / mad_noise\n",
    "\n",
    "\n",
    "def get_best_site_mad_noise(we, max_chids, unit):\n",
    "\n",
    "    # get waveforms\n",
    "    wv, _ = we.get_waveforms(unit_id=unit, with_index=True)\n",
    "\n",
    "    # get channel ids (sparse)\n",
    "    c_ids = we.sparsity.unit_id_to_channel_ids[unit]\n",
    "\n",
    "    # get nearest channel\n",
    "    max_chid = max_chids[unit]\n",
    "    max_chid_ix = np.where(c_ids == max_chid)[0][0]\n",
    "    return wv[:, :, max_chid_ix].flatten()\n",
    "\n",
    "\n",
    "def get_waveform_amplitudes_per_unit(We):\n",
    "    return si.postprocessing.compute_spike_amplitudes(We, peak_sign=\"neg\", outputs=\"by_unit\")[0]\n",
    "\n",
    "\n",
    "# SIMULATE SYNTHETIC RECORDING WITH MEAREC\n",
    "\n",
    "def simulate_synthetic_recording_with_mearec(rec_prms: dict, layer:str, cells_path, templates_path, recording_clone_h5_path, recording_clone_path, sorting_gt_clone_path, n_jobs, chunks):\n",
    "\n",
    "    # generate simulated recording (takes 22 min)\n",
    "    rec_gen = mr.gen_recordings(params=rec_prms, templates=templates_path)\n",
    "\n",
    "    # save simulated recording\n",
    "    mr.save_recording_generator(rec_gen, recording_clone_path)\n",
    "\n",
    "    # load recording h5 file\n",
    "    RecordingClone = se.MEArecRecordingExtractor(recording_clone_h5_path) \n",
    "    RecordingClone.set_property(\"layers\", np.array([layer]*384)) # add layer metadata\n",
    "\n",
    "    # load ground truth\n",
    "    SortingClone = se.MEArecSortingExtractor(recording_clone_h5_path)\n",
    "\n",
    "    # save recording extractors\n",
    "    RecordingClone.save(folder=recording_clone_path, n_jobs=n_jobs, \n",
    "                verbose=True, progress_bar=True, overwrite=True, \n",
    "                dtype=\"float32\", chunk_size=chunks)\n",
    "    \n",
    "    # save sorting extractors\n",
    "    SortingClone.save(folder=sorting_gt_clone_path, overwrite=True)\n",
    "    return RecordingClone, SortingClone, rec_gen\n",
    "\n",
    "\n",
    "def report_simulation_parameters(rec_gen):\n",
    "    \"\"\"\n",
    "    Reports simulation parameters, checks for bursting, and displays example cell metadata.\n",
    "\n",
    "    Args:\n",
    "        rec_gen (RecordingGenerator): The recording generator object.\n",
    "    \"\"\"\n",
    "    # report parameters\n",
    "    display('Simulation parameters:', rec_gen.info)\n",
    "\n",
    "    # check bursting\n",
    "    bursting = []\n",
    "    for ix in range(250):\n",
    "        bursting.append(rec_gen.spiketrains[1].annotations['bursting'])\n",
    "    print('Bursting:', any(bursting))\n",
    "\n",
    "    # example of a cell metadata\n",
    "    CELL_IDX = 0\n",
    "    print('\\nExample cell type:', rec_gen.template_celltypes[CELL_IDX])\n",
    "    display('Its spiking metadata:', rec_gen.spiketrains[CELL_IDX].annotations)\n",
    "    print('Its spike times:', rec_gen.spiketrains[CELL_IDX].times)\n",
    "\n",
    "\n",
    "# QUALITY METRICS FOR A SINGLE UNIT\n",
    "\n",
    "def get_spatial_spread(\n",
    "    We, unit_id: int, max_chids: dict, channel_ids, channel_coord\n",
    "):\n",
    "    \"\"\"measure unit's spatial spread\n",
    "\n",
    "    Args:\n",
    "        unit_id (int): _description_\n",
    "        max_chids (dict): _description_\n",
    "        Recording (_type_): _description_\n",
    "        channel_ids (_type_): _description_\n",
    "        channel_coord (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # get waveforms\n",
    "    wv, _ = We.get_waveforms(unit_id=unit_id, with_index=True)    \n",
    "\n",
    "    # get the site ids (sparse)\n",
    "    c_ids = We.sparsity.unit_id_to_channel_ids[unit_id]\n",
    "\n",
    "    # get the nearest site\n",
    "    max_chid = max_chids[unit_id]\n",
    "    max_chid_ix = np.where(c_ids == max_chid)[0][0]\n",
    "    \n",
    "    # get the average spike on each site\n",
    "    mean_spikes = wv.mean(axis=0)\n",
    "    max_spike = mean_spikes[:, max_chid_ix]\n",
    "    \n",
    "    # measure the average spike's similarity \n",
    "    # of each site to the average spike\n",
    "    # of the nearest site to the unit\n",
    "    # (ratio between 0 and 1)\n",
    "    n_sites = mean_spikes.shape[1]\n",
    "    cosim_weights = []\n",
    "    for s_i in range(n_sites):\n",
    "        cosim_weights.append(\n",
    "            np.dot(max_spike, mean_spikes[:, s_i])\n",
    "            / (lalgnorm(max_spike) * lalgnorm(mean_spikes[:, s_i]))\n",
    "        )\n",
    "    cosim_weights = np.array(cosim_weights)\n",
    "\n",
    "    # threshold the similarity metric to be positive\n",
    "    # we only look at similarity (not inverse similarity (<0))\n",
    "    cosim_weights[cosim_weights < 0] = 0\n",
    "\n",
    "    # measure the distance of the site to the nearest site\n",
    "    # to the unit\n",
    "    channel_coord = channel_coord[np.isin(channel_ids, c_ids), :]\n",
    "    try:\n",
    "        max_chids_coord = channel_coord[max_chid_ix, :]\n",
    "    except:\n",
    "        from ipdb import set_trace; set_trace()\n",
    "        \n",
    "    dist = []\n",
    "    for ix, _ in enumerate(c_ids):\n",
    "        dist.append(_euclidean_distance(max_chids_coord, channel_coord[ix]))\n",
    "    dist = np.array(dist)\n",
    "\n",
    "    # return spatial spread\n",
    "    return {\n",
    "        \"spatial_spread\": np.dot(cosim_weights, dist),\n",
    "        \"channel_distance\": dist,\n",
    "        \"weights\": cosim_weights,\n",
    "    }\n",
    "\n",
    "\n",
    "# QUALITY METRICS / SPIKE FEATURES FOR ALL UNITS\n",
    "\n",
    "def get_firing_rates(sorting, duration_s):\n",
    "    \"\"\"\n",
    "    Plots the firing rate distribution of units in a sorting extractor.\n",
    "\n",
    "    Parameters:\n",
    "    - sorting: spikeinterface SortingExtractor\n",
    "        The sorting extractor containing spike trains.\n",
    "    - duration_s: the total duration\n",
    "    - bins: int, optional\n",
    "        Number of bins for the histogram (default is 20).\n",
    "    \"\"\"\n",
    "    unit_ids = sorting.get_unit_ids()\n",
    "    firing_rates = [\n",
    "        len(sorting.get_unit_spike_train(unit_id)) / duration_s\n",
    "        for unit_id in unit_ids\n",
    "    ]\n",
    "    return firing_rates\n",
    "\n",
    "\n",
    "def get_firing_ranges(waveform_extractor):\n",
    "\n",
    "    firing_range = sqm.compute_firing_ranges(waveform_extractor=waveform_extractor)\n",
    "    return np.array(list(firing_range.values()))\n",
    "\n",
    "\n",
    "def get_waveform_amplitude_medians(waveform_extractor):\n",
    "\n",
    "    amplitude_medians = sqm.compute_amplitude_medians(waveform_extractor=waveform_extractor)\n",
    "    return np.array(list(amplitude_medians.values()))\n",
    "\n",
    "\n",
    "def get_waveform_snr(waveform_extractor):\n",
    "    snrs = sqm.compute_snrs(waveform_extractor=waveform_extractor)\n",
    "    return np.array(list(snrs.values()))\n",
    "\n",
    "\n",
    "def get_spatial_spread_all_units(\n",
    "    recording_path: str, study_path: str, ms_before=3, ms_after=3, peak_sign=\"neg\"\n",
    "):\n",
    "    \"\"\"get all units' spatial extent metrics\n",
    "\n",
    "    Args:\n",
    "        recording_path (str): Path of the Recording Extractor\n",
    "        study_path (str): _description_\n",
    "        ms_before (float): _description_\n",
    "        ms_after (float): _description_\n",
    "\n",
    "    Returns:\n",
    "        (dict): spatial spread of each unit\n",
    "        - key: true unit id\n",
    "        - value: spatial spread\n",
    "    \n",
    "    Prerequisites:\n",
    "        - have extracted ground truth waveforms\n",
    "    \"\"\"\n",
    "    # takes 1:30 min\n",
    "\n",
    "    # get Waveform extractor\n",
    "    Recording = si.load_extractor(recording_path)\n",
    "    \n",
    "    # WvfExtractor = waveform.load(\n",
    "    #     Recording, study_path, ms_before=ms_before, ms_after=ms_after\n",
    "    # )\n",
    "    We = si.WaveformExtractor.load_from_folder(study_path)\n",
    "    \n",
    "    # get sites' distance to the max site\n",
    "    # get 3D coordinates\n",
    "    #Recording = si.load_extractor(recording_path)\n",
    "    #channel_ids = Recording.get_channel_ids()\n",
    "    Rec = si.load_extractor(recording_path)\n",
    "    channel_coord = Rec.get_probe().contact_positions\n",
    "    channel_ids = Rec.get_channel_ids()\n",
    "\n",
    "    # get channels where spike amplitude is maximal\n",
    "    max_chids = ttools.get_template_extremum_channel(\n",
    "        We, peak_sign=peak_sign)\n",
    "\n",
    "    # takes 1:30 min (1310 units)\n",
    "    spatial_spread = dict()\n",
    "    for _, unit in enumerate(We.unit_ids):\n",
    "        spatial_spread[unit] = get_spatial_spread(\n",
    "            We, unit, max_chids, channel_ids, channel_coord\n",
    "        )[\"spatial_spread\"]\n",
    "    return np.array(list(spatial_spread.values()))\n",
    "\n",
    "\n",
    "def get_mad_ratios(WeOriginal):\n",
    "    \"\"\"get mad ratio per unit. Values close to 1 indicate a good isolation of the unit.\n",
    "    i.e., the mean absolute deviation of the spike amplitude is similar to the mean absolute deviation \n",
    "    of the noise amplitude.\n",
    "\n",
    "    Args:\n",
    "        WeOriginal (WaveformExtractor): _description_\n",
    "\n",
    "    Returns:\n",
    "        np.array: mad ratio per unit\n",
    "    \"\"\"\n",
    "    # pre-compute negative spike amplitudes\n",
    "    amplitudes_by_unit = get_waveform_amplitudes_per_unit(WeOriginal)\n",
    "\n",
    "    # get nearest channels\n",
    "    max_chids = ttools.get_template_extremum_channel(WeOriginal, peak_sign=\"both\")\n",
    "\n",
    "    # compute mad ratio per unit\n",
    "    mad_ratio = []\n",
    "    for unit in WeOriginal.unit_ids:\n",
    "        noise_amp = get_best_site_mad_noise(WeOriginal, max_chids, unit)\n",
    "        mad_ratio.append(get_mad_ratio(amplitudes_by_unit[unit], noise_amp))\n",
    "    return np.array(mad_ratio)\n",
    "\n",
    "\n",
    "# PLOTS \n",
    "\n",
    "def plot_firing_rate_distribution(ax, firing_rates, bins=20, title:str='simulation_name', ylim=(0,60), xlim=(0,25)):\n",
    "    \"\"\"\n",
    "    Plots the firing rate distribution of units in a sorting extractor.\n",
    "\n",
    "    Parameters:\n",
    "    - firing rates: firing rates\n",
    "    - bins: int, optional\n",
    "        Number of bins for the histogram (default is 20).\n",
    "    \"\"\"\n",
    "    ax.hist(firing_rates, bins=bins, edgecolor='k')\n",
    "    ax.set_xlabel(\"Firing rate (Hz)\")\n",
    "    ax.set_ylabel(\"Unit count\")\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim(xlim)    \n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_firing_ranges_distribution(ax, firing_ranges, bins=20, title:str='simulation_name', ylim=(0,50), xlim=(0,800)):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - amplitudes: waveform amplitudes\n",
    "    - bins: int, optional\n",
    "        Number of bins for the histogram (default is 20).\n",
    "    \"\"\"\n",
    "    ax.hist(firing_ranges, bins=bins, edgecolor='k')\n",
    "    ax.set_xlabel(\"Firing range (Hz)\")\n",
    "    ax.set_ylabel(\"Unit count\")\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_waveform_amplitude_median_distribution(ax, amplitudes, bins=20, title:str='simulation_name', ylim=(0,50), xlim=(0,800)):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - amplitudes: waveform amplitudes\n",
    "    - bins: int, optional\n",
    "        Number of bins for the histogram (default is 20).\n",
    "    \"\"\"\n",
    "    ax.hist(amplitudes, bins=bins, edgecolor='k')\n",
    "    ax.set_xlabel(\"Amplitudes (uV)\")\n",
    "    ax.set_ylabel(\"Unit count\")\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_waveform_snr_distribution(ax, snrs, bins=20, title:str='simulation_name', ylim=(0,50), xlim=(0,800)):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - snrs: snrs\n",
    "    - bins: int, optional\n",
    "        Number of bins for the histogram (default is 20).\n",
    "    \"\"\"\n",
    "    ax.hist(snrs, bins=bins, edgecolor='k')\n",
    "    ax.set_xlabel(\"SNR\")\n",
    "    ax.set_ylabel(\"Unit count\")\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_spatial_spread_distribution(ax, mad_ratio, bins=20, title:str='simulation_name', ylim=(0,50), xlim=(0,800)):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - mad_ratio: mad_ratio\n",
    "    - bins: int, optional\n",
    "        Number of bins for the histogram (default is 20).\n",
    "    \"\"\"\n",
    "    ax.hist(mad_ratio, bins=bins, edgecolor='k')\n",
    "    ax.set_xlabel(\"Spatial spread (a.u)\")\n",
    "    ax.set_ylabel(\"Unit count\")\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_mad_ratio_distribution(ax, mad_ratios, bins=20, title:str='simulation_name', ylim=(0,50), xlim=(0,800)):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - mad_ratios: waveform mean-absolute deviation ratio between spike amplitude and background noise\n",
    "    - bins: int, optional\n",
    "        Number of bins for the histogram (default is 20).\n",
    "    \"\"\"\n",
    "    ax.hist(mad_ratios, bins=bins, edgecolor='k')\n",
    "    ax.set_xlabel(\"MAD ratios (a.u)\")\n",
    "    ax.set_ylabel(\"Unit count\")\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Quality of unit isolation (single-unit yield)\n",
    "\n",
    "def process_each_experiment(sorted1, sorted2):\n",
    "    \"\"\"\n",
    "    Process sorting data by retrieving, filtering, and standardizing unit data.\n",
    "    Keep units in L1 to L6 (in cortex)\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing processed DataFrames for sorting data.\n",
    "    \"\"\"\n",
    "    # get unit data\n",
    "    sorting_data = {\n",
    "        f\"s_original\": fr.get_sorted_unit_meta(sorted1),\n",
    "        f\"s_clone\": fr.get_sorted_unit_meta(sorted2),\n",
    "    }\n",
    "\n",
    "    # standardize layer 2/3\n",
    "    for key in sorting_data:\n",
    "        sorting_data[key][\"layer\"][\n",
    "            (sorting_data[key][\"layer\"] == \"L2\") | (sorting_data[key][\"layer\"] == \"L3\") | (sorting_data[key][\"layer\"] == \"L2_3\")\n",
    "        ] = \"L2/3\"\n",
    "\n",
    "    # keep units in cortex\n",
    "    for key in sorting_data:\n",
    "        sorting_data[key] = sorting_data[key][\n",
    "            np.isin(sorting_data[key][\"layer\"], [\"L1\", \"L2\", \"L2/3\", \"L4\", \"L5\", \"L6\"])\n",
    "        ]\n",
    "    return sorting_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate intermediate synthetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setup common cell template, probe, seed parameters\n",
    "# tpl_prms = mr.get_default_templates_params()\n",
    "# tpl_prms['probe'] = 'Neuropixels-384'           # neuropixel probe\n",
    "# tpl_prms['n'] = 200                             # number of templates per cell model\n",
    "# tpl_prms['seed'] = 0                            # setup reproducibility\n",
    "\n",
    "# # generate and save the templates (10 min for 200 templates per cell, 15MB)\n",
    "# tpl_gen = mr.gen_templates(cell_models_folder=cells_path, params=tpl_prms)\n",
    "# mr.save_template_generator(tpl_gen, templates_path)\n",
    "# print(\"Cells path:\", cells_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "rec_prms = mr.get_default_recordings_params()\n",
    "rec_prms['spiketrains']['n_exc'] = 200\n",
    "rec_prms['spiketrains']['n_inh'] = 50\n",
    "rec_prms['spiketrains']['duration'] = 600\n",
    "rec_prms['seeds']['spiketrains'] = 0\n",
    "rec_prms['seeds']['templates'] = 1\n",
    "rec_prms['seeds']['noise'] = 2\n",
    "rec_prms['seeds']['convolution'] = 3\n",
    "\n",
    "# simulate synthetic recording with MEArec\n",
    "RecordingClone, SortingClone, RecGen = simulate_synthetic_recording_with_mearec(rec_prms, 'L5', CELLS_PATH, TEMPLATES_PATH, RECORDING_CLONE_H5_PATH, RECORDING_CLONE_PATH, SORTING_GT_CLONE_PATH, N_JOBS, CHUNKS)\n",
    "\n",
    "# report parameters\n",
    "report_simulation_parameters(RecGen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "rec_prms = mr.get_default_recordings_params()\n",
    "rec_prms['spiketrains']['n_exc'] = 200  # number of excitatory cells\n",
    "rec_prms['spiketrains']['n_inh'] = 50   # number of inhibitory cells\n",
    "rec_prms['spiketrains']['duration'] = 600\n",
    "rec_prms['seeds']['spiketrains'] = 0\n",
    "rec_prms['seeds']['templates'] = 1\n",
    "rec_prms['seeds']['noise'] = 2\n",
    "rec_prms['seeds']['convolution'] = 3\n",
    "\n",
    "# modified parameters\n",
    "rec_prms['spiketrains']['f_exc'] = 5    # average firing rate of excitatory cells in Hz\n",
    "rec_prms['spiketrains']['f_inh'] = 15   # average firing rate of inhibitory cells in Hz\n",
    "\n",
    "# simulate synthetic recording with MEArec\n",
    "RecordingS2, SortingS2, RecGenS2 = simulate_synthetic_recording_with_mearec(rec_prms, 'L5', CELLS_PATH_S2, TEMPLATES_PATH, RECORDING_S2_H5_PATH, RECORDING_S2_PATH, SORTING_GT_S2_CLONE_PATH, N_JOBS, CHUNKS)\n",
    "\n",
    "# report parameters\n",
    "report_simulation_parameters(RecGenS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/compute data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load orginal Buccino (already bandpass filtered 300 - 6000 Hz)\n",
    "RecordingBuccino = si.load_extractor('dataset/01_intermediate/preprocessing/recording_buccino/')\n",
    "SortingGtBuccino = si.load_extractor('dataset/00_raw/ground_truth_buccino/')\n",
    "\n",
    "# load Buccino clone (already bandpass filtered 300 - 6000 Hz)\n",
    "RecordingClone = si.load_extractor('dataset/00_raw/recording_buccino_clone/')\n",
    "SortingGtClone = si.load_extractor('dataset/00_raw/ground_truth_buccino_clone/')\n",
    "\n",
    "# load npx_spont (preprocessed high-pass filtered >300 Hz)\n",
    "RecordingNS = si.load_extractor(RECORDING_NS_PATH)\n",
    "SortingGtNS = si.load_extractor(GROUND_TRUTH_NS_PATH)\n",
    "SortingGtNS = si.curation.remove_excess_spikes(SortingGtNS, RecordingNS)\n",
    "SortingGtNS = SortingGtNS.frame_slice(start_frame=0, end_frame=RecordingNS.get_sampling_frequency() * DURATION_S)\n",
    "\n",
    "# extract waveforms (typically takes 20 secs)\n",
    "WeGtOriginal = extract_waveforms(RecordingBuccino, SortingGtBuccino, WAVEFORM_BUCCINO_PATH,\n",
    "    sparse=True, ms_before=3.0, ms_after=3.0, max_spikes_per_unit=500, unit_batch_size=200,\n",
    "    overwrite=True, seed=0, **job_kwargs)\n",
    "\n",
    "WeGtClone = extract_waveforms(RecordingClone, SortingGtClone, WAVEFORM_CLONE_PATH,\n",
    "    sparse=True, ms_before=3.0, ms_after=3.0, max_spikes_per_unit=500, unit_batch_size=200,\n",
    "    overwrite=True, seed=0, **job_kwargs)\n",
    "\n",
    "WeGtNS = extract_waveforms(RecordingNS, SortingGtNS, WAVEFORM_GT_NS_PATH,\n",
    "    sparse=True, ms_before=3.0, ms_after=3.0, max_spikes_per_unit=500, unit_batch_size=200,\n",
    "    overwrite=True, seed=0, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load recording h5 file\n",
    "RecordingClone = se.MEArecRecordingExtractor(RECORDING_CLONE_H5_PATH) \n",
    "RecordingClone.set_property(\"layers\", np.array(['L5']*384)) # add layer metadata\n",
    "\n",
    "# load ground truth\n",
    "SortingClone = se.MEArecSortingExtractor(RECORDING_CLONE_H5_PATH)\n",
    "\n",
    "# save recording extractors\n",
    "RecordingClone.save(folder=RECORDING_CLONE_PATH, n_jobs=N_JOBS, \n",
    "            verbose=True, progress_bar=True, overwrite=True, \n",
    "            dtype=\"float32\", chunk_size=CHUNKS)\n",
    "\n",
    "# save sorting extractors\n",
    "SortingClone.save(folder=GROUND_TRUTH_CLONE_PATH, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute/Plot quality features\n",
    "\n",
    "We can match:\n",
    "\n",
    "- nb of cells\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# setup  plot \n",
    "fig, axes = plt.subplots(6,3, figsize=(5,5))\n",
    "\n",
    "# CELL SPIKING FEATURES ---------------\n",
    "\n",
    "\n",
    "# firing rates\n",
    "firing_rates = get_firing_rates(SortingGtBuccino, duration_s=600) # original Buccino\n",
    "plot_firing_rate_distribution(axes[0,0], firing_rates, title='Original Buccino', ylim=(0,60), xlim=(0,25))\n",
    "\n",
    "firing_rates = get_firing_rates(SortingGtClone, duration_s=600) # our Buccino clone\n",
    "plot_firing_rate_distribution(axes[0,1], firing_rates, title='Buccino clone', ylim=(0,60), xlim=(0,25))\n",
    "\n",
    "firing_rates = get_firing_rates(SortingGtNS, duration_s=600) # npx_spont\n",
    "plot_firing_rate_distribution(axes[0,2], firing_rates, title='NS', ylim=(0,1000), xlim=(0,25))\n",
    "\n",
    "\n",
    "# firing range\n",
    "firing_ranges = get_firing_ranges(WeGtOriginal)\n",
    "plot_firing_ranges_distribution(axes[1,0], firing_ranges, title='', ylim=(0,60), xlim=(0,8))\n",
    "\n",
    "firing_ranges = get_firing_ranges(WeGtClone)\n",
    "plot_firing_ranges_distribution(axes[1,1], firing_ranges, title='', ylim=(0,60), xlim=(0,8))\n",
    "\n",
    "firing_ranges = get_firing_ranges(WeGtNS)\n",
    "plot_firing_ranges_distribution(axes[1,2], firing_ranges, title='', ylim=(0,1000), xlim=(0,8))\n",
    "\n",
    "\n",
    "# CELL WAVEFORM FEATURES ---------------\n",
    "\n",
    "# waveform amplitudes\n",
    "amplitudes_o = get_waveform_amplitude_medians(WeGtOriginal)\n",
    "plot_waveform_amplitude_median_distribution(axes[2,0], amplitudes_o, title='', ylim=(0,65), xlim=(0,600))\n",
    "\n",
    "amplitudes_c = get_waveform_amplitude_medians(WeGtClone)\n",
    "plot_waveform_amplitude_median_distribution(axes[2,1], amplitudes_c, title='', ylim=(0,65), xlim=(0,600))\n",
    "\n",
    "amplitudes_ns = get_waveform_amplitude_medians(WeGtNS)\n",
    "plot_waveform_amplitude_median_distribution(axes[2,2], amplitudes_ns, title='', ylim=(0,1000), xlim=(0,600))\n",
    "\n",
    "\n",
    "# signal-to-noise ratio (SNR)\n",
    "snrs = get_waveform_snr(WeGtOriginal)\n",
    "plot_waveform_snr_distribution(axes[3,0], snrs, title='', ylim=(0,65), xlim=(0,70))\n",
    "\n",
    "snrs = get_waveform_snr(WeGtClone)\n",
    "plot_waveform_snr_distribution(axes[3,1], snrs, title='', ylim=(0,65), xlim=(0,70))\n",
    "\n",
    "snrs = get_waveform_snr(WeGtNS)\n",
    "plot_waveform_snr_distribution(axes[3,2], snrs[np.isfinite(snrs)], title='', ylim=(0,1000), xlim=(0,70))\n",
    "\n",
    "\n",
    "# spatial spread\n",
    "spreads = get_spatial_spread_all_units(RECORDING_BUCCINO_PATH, WAVEFORM_GT_BUCCINO_PATH, ms_before=3, ms_after=3, peak_sign=\"neg\")\n",
    "plot_spatial_spread_distribution(axes[4,0], spreads, title='', ylim=(0,40), xlim=(0,1200))\n",
    "\n",
    "spreads = get_spatial_spread_all_units(RECORDING_CLONE_PATH, WAVEFORM_GT_CLONE_PATH, ms_before=3, ms_after=3, peak_sign=\"neg\")\n",
    "plot_spatial_spread_distribution(axes[4,1], spreads, title='', ylim=(0,40), xlim=(0,1200))\n",
    "\n",
    "spreads = get_spatial_spread_all_units(RECORDING_NS_PATH, WAVEFORM_GT_NS_PATH, ms_before=3, ms_after=3, peak_sign=\"neg\")\n",
    "plot_spatial_spread_distribution(axes[4,2], spreads, title='', ylim=(0,300), xlim=(0,1200))\n",
    "\n",
    "\n",
    "# mad ratio\n",
    "mad_ratios = get_mad_ratios(WeGtOriginal)\n",
    "plot_mad_ratio_distribution(axes[5,0], mad_ratios, title='', ylim=(0,40), xlim=(0,15))\n",
    "\n",
    "mad_ratios = get_mad_ratios(WeGtClone)\n",
    "plot_mad_ratio_distribution(axes[5,1], mad_ratios, title='', ylim=(0,40), xlim=(0,15))\n",
    "\n",
    "mad_ratios = get_mad_ratios(WeGtNS)\n",
    "plot_mad_ratio_distribution(axes[5,2], mad_ratios, title='', ylim=(0,800), xlim=(0,15))\n",
    "\n",
    "\n",
    "# CELL POSITIONS -----------------------\n",
    "\n",
    "# distances to electrode\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SORTED_ks4_paths = {\n",
    "    \"s_original\": 'dataset/01_intermediate/sorting/buccino/SortingKS4',\n",
    "    \"s_clone\": './temp/npx_synth_clone/SortingKS4_10m_RTX5090',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "exp = 'Synthetic'\n",
    "\n",
    "# compute data\n",
    "plot_data = process_each_experiment(SORTED_ks4_paths['s_original'], SORTED_ks4_paths['s_clone'])\n",
    "\n",
    "# setup\n",
    "FIGSIZE = (2.5, 2)\n",
    "SHIFT = 0.3\n",
    "exp_names = ('Original', 'Clone')\n",
    "number_pos = {'exp1_x':-0.3 + SHIFT, 'exp1_y_su': 0.2, 'exp1_y_mu': 0.8, \n",
    "              'exp2_x': 0.7 + SHIFT, 'exp2_y_su': 0.2, 'exp2_y_mu': 0.8}\n",
    "legend_cfg = {\"frameon\": False, \"handletextpad\": 0.1}\n",
    "\n",
    "# plot (white: single-unit; black: multi-units)\n",
    "_, ax = plt.subplots(1,1,figsize=FIGSIZE)\n",
    "isolation.plot_unit_isolation_pros_of_added_detailed(ax, plot_data['s_original'], plot_data['s_clone'], legend_cfg, number_pos, exp_names)\n",
    "\n",
    "# legend\n",
    "ax.set_yticks([0, 0.25, 0.5, 0.75, 1], [0, 0.25, 0.5, 0.75, 1])\n",
    "ax.set_title(exp)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SortingGtNS = si.load_extractor('dataset/00_raw/sorting_npx_spont/')\n",
    "SortingGtNS.get_property_keys()\n",
    "\n",
    "GroundTruth = si.load_extractor('dataset/00_raw/ground_truth_npx_spont/')\n",
    "GroundTruth.get_property_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean up default temporary data - you will need to reinstall mearec afterward with pip intall mearec\n",
    "# import shutil\n",
    "# shutil.rmtree(CELLS_PATH, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
