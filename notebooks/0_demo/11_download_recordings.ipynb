{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Download full recordings\n",
    "\n",
    "author: laquitainesteeve@gmail.com\n",
    "\n",
    "purpose: download all full recordings from Dandi Archive\n",
    "\n",
    "Description:\n",
    "* model: biophysical simulation\n",
    "* duration: 34 min\n",
    "* size: 100 GB\n",
    "* layers: 6 layers: L1, L2/3, L4, L5, L6\n",
    "* noise: background noise fitted to Marques-Smith\n",
    "* network state: spontaneous\n",
    "\n",
    "Execution time: 3h40 just for 30 min of 384 channels\n",
    "* note: writing speed: maximize chunk_size, set to n_jobs=20. This maximizes speed while avoiding overhead [1].\n",
    "\n",
    "Hardware: CPU\n",
    "\n",
    "Tested on: 32 cores, 2TB storage, 188GB RAM Ubuntu machine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup \n",
    "\n",
    "Activate virtual environment (envs/spikebias.yml)\n",
    "\n",
    "```bash\n",
    "python -m ipykernel install --user --name dandi --display-name \"dandi\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spikeinterface 0.101.2\n",
      "CPU times: user 1.02 s, sys: 1.89 s, total: 2.92 s\n",
      "Wall time: 370 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steeve/steeve/epfl/code/spikebias/envs/dandi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# import python packages\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "import spikeinterface.extractors as se\n",
    "print(\"spikeinterface\", spikeinterface.__version__)\n",
    "\n",
    "# set the project path\n",
    "PROJ_PATH = \"/home/steeve/steeve/epfl/code/spikebias\"\n",
    "\n",
    "# set the raw dataset path\n",
    "RAW_DATASET = os.path.join(PROJ_PATH, \"dataset/00_raw/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Data loader for dandi datasets\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_dataset_path:str, dandiset_id:str, filepath:str, is_recording=True, is_sorting=True):\n",
    "        self.raw_dataset_path = raw_dataset_path\n",
    "        self.dandiset_id = dandiset_id\n",
    "        self.filepath = filepath\n",
    "        self.is_recording = is_recording\n",
    "        self.is_sorting = is_sorting        \n",
    "        self.recording = None\n",
    "        self.sorting = None\n",
    "        self.s3_path = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \n",
    "        # Get the file path on S3\n",
    "        with DandiAPIClient() as client:\n",
    "            asset = client.get_dandiset(self.dandiset_id, 'draft').get_asset_by_path(self.filepath)\n",
    "            self.s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "        print(\"s3_path:\", self.s3_path)\n",
    "\n",
    "        # Get RecordingExtractor and SortingExtractor\n",
    "        if self.is_recording:\n",
    "            self.recording = se.NwbRecordingExtractor(file_path=self.s3_path, stream_mode=\"remfile\")\n",
    "        if self.is_sorting:\n",
    "            self.sorting = se.NwbSortingExtractor(file_path=self.s3_path, stream_mode=\"remfile\")\n",
    "\n",
    "        # Report\n",
    "        print('\\nDownloaded recording:', self.recording)\n",
    "        print('\\nDownloaded sorting:', self.sorting)\n",
    "\n",
    "    def save_data(self, recording_folder:str, sorting_folder:str, n_jobs=30, chunk_size=800000, dtype='float32', duration_secs=None):\n",
    "        if self.is_recording:\n",
    "            if duration_secs: \n",
    "                self.recording = self.recording.frame_slice(start_frame=0, end_frame=self.recording.sampling_frequency*duration_secs)\n",
    "            self.recording.save(folder=recording_folder, n_jobs=n_jobs, verbose=True, progress_bar=True, overwrite=True, dtype=dtype, chunk_size=chunk_size)                \n",
    "\n",
    "        if self.is_sorting:\n",
    "            if duration_secs: \n",
    "                self.sorting = self.sorting.frame_slice(start_frame=0, end_frame=self.recording.sampling_frequency*duration_secs)\n",
    "            self.sorting.save(folder=sorting_folder, progress_bar=True, overwrite=True)\n",
    "        \n",
    "        # Report\n",
    "        print('\\nSaved recording:', self.recording)\n",
    "        print('\\nSaved sorting:', self.sorting)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPX spont biophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/0c3/2c4/0c32c475-1251-485b-9934-83667b3ba4ba\n",
      "\n",
      " NwbRecordingExtractor: 384 channels - 40.0kHz - 1 segments - 82,319,958 samples \n",
      "                       2,058.00s (34.30 minutes) - float32 dtype - 117.76 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/0c3/2c4/0c32c475-1251-485b-9934-83667b3ba4ba\n",
      "\n",
      " NwbSortingExtractor: 1388 units - 1 segments - 40.0kHz\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/0c3/2c4/0c32c475-1251-485b-9934-83667b3ba4ba\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=1.14 GiB - total_memory=34.33 GiB - chunk_duration=20.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 103/103 [3:33:22<00:00, 124.29s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.03 s, sys: 773 ms, total: 1.8 s\n",
      "Wall time: 3h 34min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-001-fitted/sub-001-fitted_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_npx_spont\")\n",
    "sorting_folder = os.path.join(RAW_DATASET, \"sorting_npx_spont\")\n",
    "\n",
    "# download and save dataset (3h40)\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPX evoked biophy\n",
    "\n",
    "- Execution time: 14 min for 10 min recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/9d6/6ed/9d66ed40-af31-43aa-b4ba-246d2206dcad\n",
      "\n",
      "Downloaded recording: NwbRecordingExtractor: 384 channels - 20.0kHz - 1 segments - 72,359,964 samples \n",
      "                       3,618.00s (1.00 hours) - float32 dtype - 103.51 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/9d6/6ed/9d66ed40-af31-43aa-b4ba-246d2206dcad\n",
      "\n",
      "Downloaded sorting: NwbSortingExtractor: 1836 units - 1 segments - 20.0kHz\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/9d6/6ed/9d66ed40-af31-43aa-b4ba-246d2206dcad\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=1.14 GiB - total_memory=34.33 GiB - chunk_duration=40.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 23/23 [25:01<00:00, 65.29s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved recording: FrameSliceRecording: 384 channels - 20.0kHz - 1 segments - 18,000,000 samples \n",
      "                     900.00s (15.00 minutes) - float32 dtype - 25.75 GiB\n",
      "\n",
      "Saved sorting: FrameSliceSorting: 1836 units - 1 segments - 20.0kHz\n",
      "CPU times: user 870 ms, sys: 379 ms, total: 1.25 s\n",
      "Wall time: 25min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-002-fitted/sub-002-fitted_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_npx_evoked\")\n",
    "sorting_folder = os.path.join(RAW_DATASET, \"sorting_npx_evoked\")\n",
    "DURATION_SECS = 900 # 15 min\n",
    "\n",
    "# download and save dataset\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32', duration_secs=DURATION_SECS) # save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='border:1px solid #ddd; padding:10px;'><strong>FrameSliceRecording: 384 channels - 40.0kHz - 1 segments - 24,000,000 samples - 600.00s (10.00 minutes) - float32 dtype - 34.33 GiB</strong></div><details style='margin-left: 10px;'>  <summary><strong>Channel IDs</strong></summary><ul>['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15'\n",
       " '16' '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29'\n",
       " '30' '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43'\n",
       " '44' '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57'\n",
       " '58' '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71'\n",
       " '72' '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85'\n",
       " '86' '87' '88' '89' '90' '91' '92' '93' '94' '95' '96' '97' '98' '99'\n",
       " '100' '101' '102' '103' '104' '105' '106' '107' '108' '109' '110' '111'\n",
       " '112' '113' '114' '115' '116' '117' '118' '119' '120' '121' '122' '123'\n",
       " '124' '125' '126' '127' '128' '129' '130' '131' '132' '133' '134' '135'\n",
       " '136' '137' '138' '139' '140' '141' '142' '143' '144' '145' '146' '147'\n",
       " '148' '149' '150' '151' '152' '153' '154' '155' '156' '157' '158' '159'\n",
       " '160' '161' '162' '163' '164' '165' '166' '167' '168' '169' '170' '171'\n",
       " '172' '173' '174' '175' '176' '177' '178' '179' '180' '181' '182' '183'\n",
       " '184' '185' '186' '187' '188' '189' '190' '191' '192' '193' '194' '195'\n",
       " '196' '197' '198' '199' '200' '201' '202' '203' '204' '205' '206' '207'\n",
       " '208' '209' '210' '211' '212' '213' '214' '215' '216' '217' '218' '219'\n",
       " '220' '221' '222' '223' '224' '225' '226' '227' '228' '229' '230' '231'\n",
       " '232' '233' '234' '235' '236' '237' '238' '239' '240' '241' '242' '243'\n",
       " '244' '245' '246' '247' '248' '249' '250' '251' '252' '253' '254' '255'\n",
       " '256' '257' '258' '259' '260' '261' '262' '263' '264' '265' '266' '267'\n",
       " '268' '269' '270' '271' '272' '273' '274' '275' '276' '277' '278' '279'\n",
       " '280' '281' '282' '283' '284' '285' '286' '287' '288' '289' '290' '291'\n",
       " '292' '293' '294' '295' '296' '297' '298' '299' '300' '301' '302' '303'\n",
       " '304' '305' '306' '307' '308' '309' '310' '311' '312' '313' '314' '315'\n",
       " '316' '317' '318' '319' '320' '321' '322' '323' '324' '325' '326' '327'\n",
       " '328' '329' '330' '331' '332' '333' '334' '335' '336' '337' '338' '339'\n",
       " '340' '341' '342' '343' '344' '345' '346' '347' '348' '349' '350' '351'\n",
       " '352' '353' '354' '355' '356' '357' '358' '359' '360' '361' '362' '363'\n",
       " '364' '365' '366' '367' '368' '369' '370' '371' '372' '373' '374' '375'\n",
       " '376' '377' '378' '379' '380' '381' '382' '383'] </details><details style='margin-left: 10px;'>  <summary><strong>Annotations</strong></summary><ul><li> <strong> is_filtered </strong>: False</li></ul> </details><details style='margin-left: 10px;'><summary><strong>Channel Properties</strong></summary><ul><details><summary> <strong> gain_to_uV </strong> </summary>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]</details><details><summary> <strong> offset_to_uV </strong> </summary>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</details><details><summary> <strong> group </strong> </summary>['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
       " '0' '0' '0' '0' '0' '0']</details><details><summary> <strong> brain_area </strong> </summary>['unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown'\n",
       " 'unknown' 'unknown' 'unknown' 'unknown' 'unknown' 'unknown']</details><details><summary> <strong> layers </strong> </summary>['Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6'\n",
       " 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5'\n",
       " 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L4' 'L4' 'L4' 'L4' 'L4' 'L3' 'L3' 'L3'\n",
       " 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L2' 'L2' 'L1' 'L1' 'L1' 'L1' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6'\n",
       " 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5'\n",
       " 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L4' 'L4' 'L4' 'L4' 'L4' 'L3' 'L3' 'L3'\n",
       " 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L2' 'L2' 'L2' 'L1' 'L1' 'L1' 'L1'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6'\n",
       " 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L5' 'L5' 'L5'\n",
       " 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L4' 'L4' 'L4' 'L4'\n",
       " 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L2' 'L2' 'L1' 'L1'\n",
       " 'L1' 'L1' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6'\n",
       " 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L6' 'L5' 'L5' 'L5'\n",
       " 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L5' 'L4' 'L4' 'L4' 'L4'\n",
       " 'L4' 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L3' 'L2' 'L2' 'L2' 'L1'\n",
       " 'L1' 'L1' 'L1' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside' 'Outside'\n",
       " 'Outside' 'Outside' 'Outside' 'Outside']</details><details><summary> <strong> location </strong> </summary>[[ 2744.8337607   -881.07702126 -3870.74470737]\n",
       " [ 2770.135404    -889.29972517 -3840.87475614]\n",
       " [ 2795.4370473   -897.52242907 -3811.0048049 ]\n",
       " ...\n",
       " [ 5125.37300183 -1604.2500257  -1077.90426669]\n",
       " [ 5150.67464513 -1612.4727296  -1048.03431545]\n",
       " [ 5175.97628843 -1620.69543351 -1018.16436422]]</details></ul></details>"
      ],
      "text/plain": [
       "FrameSliceRecording: 384 channels - 40.0kHz - 1 segments - 24,000,000 samples \n",
       "                     600.00s (10.00 minutes) - float32 dtype - 34.33 GiB"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spikeinterface as si\n",
    "Recording = si.load_extractor('/home/steeve/steeve/epfl/code/spikebias/dataset/00_raw/recording_npx_spont')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense depth 1 biophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/dec/e65/dece6568-cee4-4ade-80bf-c1166a03fe2a\n",
      "\n",
      " NwbRecordingExtractor: 128 channels - 20.0kHz - 1 segments - 34,299,965 samples \n",
      "                       1,715.00s (28.58 minutes) - float32 dtype - 16.36 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/dec/e65/dece6568-cee4-4ade-80bf-c1166a03fe2a\n",
      "\n",
      " NwbSortingExtractor: 287 units - 1 segments - 20.0kHz\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/dec/e65/dece6568-cee4-4ade-80bf-c1166a03fe2a\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=390.62 MiB - total_memory=11.44 GiB - chunk_duration=40.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 43/43 [07:57<00:00, 11.10s/it]  \n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-003-fitted/sub-003-fitted_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_dense_probe1\")\n",
    "sorting_folder = os.path.join(RAW_DATASET, \"sorting_dense_probe1\")\n",
    "\n",
    "# download and save dataset\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense depth 2 biophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/eef/9e9/eef9e95c-fb5b-46d2-a24c-878d8170b5e0\n",
      "\n",
      " NwbRecordingExtractor: 128 channels - 20.0kHz - 1 segments - 23,519,976 samples \n",
      "                       1,176.00s (19.60 minutes) - float32 dtype - 11.22 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/eef/9e9/eef9e95c-fb5b-46d2-a24c-878d8170b5e0\n",
      "\n",
      " NwbSortingExtractor: 770 units - 1 segments - 20.0kHz\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/eef/9e9/eef9e95c-fb5b-46d2-a24c-878d8170b5e0\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=390.62 MiB - total_memory=11.44 GiB - chunk_duration=40.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 30/30 [04:30<00:00,  9.02s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 245 ms, sys: 241 ms, total: 486 ms\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-004-fitted/sub-004-fitted_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_dense_probe2\")\n",
    "sorting_folder = os.path.join(RAW_DATASET, \"sorting_dense_probe2\")\n",
    "\n",
    "# download and save dataset\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense depth 3 biophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/ee2/816/ee2816de-d861-4b55-9cde-416a52e54049\n",
      "\n",
      " NwbRecordingExtractor: 128 channels - 20.0kHz - 1 segments - 35,279,964 samples \n",
      "                       1,764.00s (29.40 minutes) - float32 dtype - 16.82 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/ee2/816/ee2816de-d861-4b55-9cde-416a52e54049\n",
      "\n",
      " NwbSortingExtractor: 1123 units - 1 segments - 20.0kHz\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/ee2/816/ee2816de-d861-4b55-9cde-416a52e54049\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=390.62 MiB - total_memory=11.44 GiB - chunk_duration=40.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 45/45 [07:44<00:00, 10.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 313 ms, sys: 216 ms, total: 529 ms\n",
      "Wall time: 7min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-005-fitted/sub-005-fitted_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_dense_probe3\")\n",
    "sorting_folder = os.path.join(RAW_DATASET, \"sorting_dense_probe3\")\n",
    "\n",
    "# download and save dataset\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marques-Smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/109/db6/109db6a7-500b-4e59-83ca-8422c27137cf\n",
      "\n",
      " NwbRecordingExtractor: 384 channels - 30.0kHz - 1 segments - 36,451,538 samples \n",
      "                       1,215.05s (20.25 minutes) - int16 dtype - 26.07 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/109/db6/109db6a7-500b-4e59-83ca-8422c27137cf\n",
      "\n",
      " None\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=585.94 MiB - total_memory=17.17 GiB - chunk_duration=26.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 46/46 [05:54<00:00,  7.70s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99 ms, sys: 189 ms, total: 288 ms\n",
      "Wall time: 5min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-vivo-marques-smith/sub-vivo-marques-smith_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_marques_smith\")\n",
    "sorting_folder = None\n",
    "\n",
    "# download and save dataset\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath, is_sorting=False)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horvath depth 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/10a/c1a/10ac1a40-6918-4eea-b80c-d887bad92ae9\n",
      "\n",
      " NwbRecordingExtractor: 128 channels - 20.0kHz - 1 segments - 72,131,040 samples \n",
      "                       3,606.55s (1.00 hours) - int16 dtype - 17.20 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/10a/c1a/10ac1a40-6918-4eea-b80c-d887bad92ae9\n",
      "\n",
      " None\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=195.31 MiB - total_memory=5.72 GiB - chunk_duration=40.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 91/91 [07:46<00:00,  5.13s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 118 ms, sys: 203 ms, total: 321 ms\n",
      "Wall time: 7min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-vivo-horvath-depth-1/sub-vivo-horvath-depth-1_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_horvath_probe1\")\n",
    "sorting_folder = None\n",
    "\n",
    "# download and save dataset\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath, is_sorting=False)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horvath depth 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/4d2/875/4d2875ba-d83b-44d5-9036-74f42e19e8a0\n",
      "\n",
      " NwbRecordingExtractor: 128 channels - 20.0kHz - 1 segments - 73,773,360 samples \n",
      "                       3,688.67s (1.02 hours) - int16 dtype - 17.59 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/4d2/875/4d2875ba-d83b-44d5-9036-74f42e19e8a0\n",
      "\n",
      " None\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=195.31 MiB - total_memory=5.72 GiB - chunk_duration=40.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 93/93 [07:55<00:00,  5.11s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 173 ms, total: 290 ms\n",
      "Wall time: 7min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-vivo-horvath-depth-2/sub-vivo-horvath-depth-2_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_horvath_probe2\")\n",
    "sorting_folder = None\n",
    "\n",
    "# download and save dataset\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath, is_sorting=False)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horvath depth 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/14a/205/14a205ea-306f-47fd-97e8-284a6f00626b\n",
      "\n",
      " NwbRecordingExtractor: 128 channels - 20.0kHz - 1 segments - 72,061,920 samples \n",
      "                       3,603.10s (1.00 hours) - int16 dtype - 17.18 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/14a/205/14a205ea-306f-47fd-97e8-284a6f00626b\n",
      "\n",
      " None\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=195.31 MiB - total_memory=5.72 GiB - chunk_duration=40.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 91/91 [07:38<00:00,  5.04s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 ms, sys: 167 ms, total: 275 ms\n",
      "Wall time: 7min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-vivo-horvath-depth-3/sub-vivo-horvath-depth-3_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_horvath_probe3\")\n",
    "sorting_folder = None\n",
    "\n",
    "# download and save dataset\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath, is_sorting=False)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/465/9c0/4659c033-b336-4b8b-b947-77434cebf494\n",
      "\n",
      " NwbRecordingExtractor: 384 channels - 32.0kHz - 1 segments - 19,200,000 samples \n",
      "                       600.00s (10.00 minutes) - float32 dtype - 27.47 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/465/9c0/4659c033-b336-4b8b-b947-77434cebf494\n",
      "\n",
      " NwbSortingExtractor: 250 units - 1 segments - 32.0kHz\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/465/9c0/4659c033-b336-4b8b-b947-77434cebf494\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=1.14 GiB - total_memory=34.33 GiB - chunk_duration=25.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 24/24 [10:01:15<00:00, 1503.14s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.52 s, sys: 448 ms, total: 1.96 s\n",
      "Wall time: 10h 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '000034'\n",
    "filepath = 'sub-MEAREC-250neuron-Neuropixels/sub-MEAREC-250neuron-Neuropixels_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_buccino\")\n",
    "sorting_folder = os.path.join(RAW_DATASET, \"sorting_buccino\")\n",
    "\n",
    "# download and save dataset\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-cell isolated traces with Reyes probe \n",
    "\n",
    "- 40 second recording\n",
    "- Execution time: 4 min\n",
    "- size: 390.62 MB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/2f3/edc/2f3edc58-b09e-4571-bb1c-1ffb2f768b57\n",
      "\n",
      "Downloaded recording: NwbRecordingExtractor: 128 channels - 20.0kHz - 1 segments - 799,900 samples - 39.99s \n",
      "                       float32 dtype - 390.58 MiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/2f3/edc/2f3edc58-b09e-4571-bb1c-1ffb2f768b57\n",
      "\n",
      "Downloaded sorting: NwbSortingExtractor: 1 units - 1 segments - 20.0kHz\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/2f3/edc/2f3edc58-b09e-4571-bb1c-1ffb2f768b57\n",
      "write_binary_recording \n",
      "n_jobs=30 - samples_per_chunk=800,000 - chunk_memory=390.62 MiB - total_memory=11.44 GiB - chunk_duration=40.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|██████████| 1/1 [04:27<00:00, 267.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved recording: NwbRecordingExtractor: 128 channels - 20.0kHz - 1 segments - 799,900 samples - 39.99s \n",
      "                       float32 dtype - 390.58 MiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/2f3/edc/2f3edc58-b09e-4571-bb1c-1ffb2f768b57\n",
      "\n",
      "Saved sorting: NwbSortingExtractor: 1 units - 1 segments - 20.0kHz\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/2f3/edc/2f3edc58-b09e-4571-bb1c-1ffb2f768b57\n",
      "CPU times: user 129 ms, sys: 33.8 ms, total: 163 ms\n",
      "Wall time: 4min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set dataset parameters\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-biophy-isolated-traces-reyes/sub-biophy-isolated-traces-reyes_ses-006_ecephys.nwb'\n",
    "recording_folder = os.path.join(RAW_DATASET, \"recording_reyes_isolated_traces\")\n",
    "sorting_folder = os.path.join(RAW_DATASET, \"sorting_reyes_isolated_traces\")\n",
    "\n",
    "# download and save dataset\n",
    "data_loader = DataLoader(raw_dataset_path=RAW_DATASET, dandiset_id=dandiset_id, filepath=filepath)\n",
    "data_loader.load_data() # Load the data\n",
    "data_loader.save_data(recording_folder=recording_folder, sorting_folder=sorting_folder, n_jobs=30, chunk_size=800000, dtype='float32') # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] https://github.com/SpikeInterface/spikeinterface/issues/3252\n",
    "* effect of n_jobs and chunk_size on writing speed:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
