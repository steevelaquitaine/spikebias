{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d22b34-dd1f-4f52-9688-8a4b8616e06c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Create demo spontaneous\n",
    "\n",
    "author: laquitainesteeve@gmail.com\n",
    "\n",
    "purpose: create small demo dataset, upload/download to/from Dandi Archive (1 min of 40 KHz spontaneous neuropixels)\n",
    "\n",
    "Description:\n",
    "* 1 min recording\n",
    "* two layers: L5, L6\n",
    "* 1.2 GB\n",
    "\n",
    "Execution time: < 10 min\n",
    "\n",
    "Special hardware: on CPU, does not require GPU.\n",
    "\n",
    "# Setup \n",
    "\n",
    "Activate dandi virtual environment (envs/dandi.yml)\n",
    "\n",
    "```bash\n",
    "python -m ipykernel install --user --name demo --display-name \"demo\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b8f812-1185-4419-ab2d-5e9879786b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steeve_laquitaine/Desktop/EPFL/2022_2024_bbp/spikebias/envs/demo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spikeinterface 0.101.2\n",
      "CPU times: user 3.15 s, sys: 606 ms, total: 3.76 s\n",
      "Wall time: 4.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# import python packages\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface\n",
    "from pynwb.file import NWBFile, Subject\n",
    "from pynwb import NWBHDF5IO\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "from neuroconv.tools.spikeinterface import add_recording_to_nwbfile, add_sorting_to_nwbfile\n",
    "print(\"spikeinterface\", spikeinterface.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4059b-ad1f-4583-b619-64a5016ac027",
   "metadata": {},
   "source": [
    "## Load dandiset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b76324a-355f-4c91-9b27-3d9f89b9ae5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/9d6/6ed/9d66ed40-af31-43aa-b4ba-246d2206dcad\n",
      "\n",
      " NwbRecordingExtractor: 384 channels - 20.0kHz - 1 segments - 72,359,964 samples \n",
      "                       3,618.00s (1.00 hours) - float32 dtype - 103.51 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/9d6/6ed/9d66ed40-af31-43aa-b4ba-246d2206dcad\n",
      "\n",
      " NwbSortingExtractor: 1836 units - 1 segments - 20.0kHz\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/9d6/6ed/9d66ed40-af31-43aa-b4ba-246d2206dcad\n",
      "CPU times: user 554 ms, sys: 63.1 ms, total: 617 ms\n",
      "Wall time: 5.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load dandiset (npx, evoked, 20Khz)\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-001-fitted/sub-001-fitted_ecephys.nwb'\n",
    "\n",
    "# get the file path on S3\n",
    "with DandiAPIClient() as client:\n",
    "    asset = client.get_dandiset(dandiset_id, 'draft').get_asset_by_path(filepath)\n",
    "    s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "print(\"s3_path:\", s3_path)\n",
    "\n",
    "# get RecordingExtractor\n",
    "Recording = se.NwbRecordingExtractor(file_path=s3_path, stream_mode=\"remfile\")\n",
    "Sorting = se.NwbSortingExtractor(file_path=s3_path, stream_mode=\"remfile\")\n",
    "\n",
    "# report\n",
    "print('\\n', Recording)\n",
    "print('\\n', Sorting)\n",
    "\n",
    "# unit-test\n",
    "assert \"layers\" in Recording.get_property_keys(), \"RecordingExtractor should contain layer property\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f098c7-ee73-4688-adfa-8e7c2d1bfe66",
   "metadata": {},
   "source": [
    "## Make the demo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7924863a-9fca-4641-92df-48feb1b0a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recording: ChannelSliceRecording: 120 channels - 20.0kHz - 1 segments - 72,359,964 samples \n",
      "                       3,618.00s (1.00 hours) - float32 dtype - 32.35 GiB\n",
      "\n",
      "Recording: FrameSliceRecording: 120 channels - 20.0kHz - 1 segments - 1,200,000 samples \n",
      "                     60.00s (1.00 minutes) - float32 dtype - 549.32 MiB\n",
      "\n",
      "Sorting: FrameSliceSorting: 1836 units - 1 segments - 20.0kHz\n"
     ]
    }
   ],
   "source": [
    "# select layer 5, 6 (most of the activity)\n",
    "selected_layers = ['L5', 'L6']\n",
    "channel_ids = Recording.channel_ids\n",
    "channel_ids = channel_ids[np.isin(Recording.get_property('layers'), selected_layers)]\n",
    "SmallRecording = Recording.channel_slice(channel_ids=channel_ids)\n",
    "print(\"\\nRecording:\", SmallRecording)\n",
    "\n",
    "# select first 2 minutes (~500 MB)\n",
    "sampling_rate = Recording.get_sampling_frequency() \n",
    "start_frame = 0\n",
    "end_frame = sampling_rate * 60\n",
    "SmallRecording = SmallRecording.frame_slice(start_frame=start_frame, end_frame=end_frame)\n",
    "SmallSorting = Sorting.frame_slice(start_frame=start_frame, end_frame=end_frame)\n",
    "\n",
    "print(\"\\nRecording:\", SmallRecording)\n",
    "print(\"\\nSorting:\", SmallSorting)\n",
    "\n",
    "# unit-test\n",
    "# - layers\n",
    "# - max spike times lower than number of frames\n",
    "assert (np.unique(SmallRecording.get_property('layers'))==selected_layers).all(), \"layers are not correct\"\n",
    "\n",
    "max_spike_time = max([SmallSorting.get_unit_spike_train(unit_id=unit).tolist() for unit in SmallSorting.get_unit_ids()])[0]\n",
    "assert max_spike_time < end_frame, \"max spike timestamp should be lower that the number of frames\"\n",
    "\n",
    "# Write [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92890ef-de11-44fd-9249-fb98f6cf526a",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6b1d7da-537b-4c67-b1bf-7755a30854ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/steevelaquitaine/spikebias/envs/dandi/lib/python3.10/site-packages/spikeinterface/core/job_tools.py:103: UserWarning: `n_jobs` is not set so parallel processing is disabled! To speed up computations, it is recommended to set n_jobs either globally (with the `spikeinterface.set_global_job_kwargs()` function) or locally (with the `n_jobs` argument). Use `spikeinterface.set_global_job_kwargs?` for more information about job_kwargs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_binary_recording \n",
      "n_jobs=1 - samples_per_chunk=20,000 - chunk_memory=9.16 MiB - total_memory=9.16 MiB - chunk_duration=1.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording: 100%|##########| 60/60 [01:17<00:00,  1.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border:1px solid #ddd; padding:10px;'><strong>NumpyFolder: 1836 units - 1 segments - 20.0kHz</strong></div><details style='margin-left: 10px;'>  <summary><strong>Unit IDs</strong></summary><ul>['12165' '15894' '16652' ... '4228700' '4229218' '4229506'] </details><details style='margin-left: 10px;'>  <summary><strong>Annotations</strong></summary><ul></details><details style='margin-left: 10px;'><summary><strong>Unit Properties</strong></summary><ul></ul></details>"
      ],
      "text/plain": [
       "NumpyFolder: 1836 units - 1 segments - 20.0kHz"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SmallRecording.save(folder=\"./demo_recording\", overwrite=True)\n",
    "SmallSorting.save(folder=\"./demo_sorting\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8304f29-a9fc-4e83-93b8-a6ff5c8bec6a",
   "metadata": {},
   "source": [
    "## Write demo as nwb and upload to dandi archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1328fecf-e4f1-4f48-adc7-62378dcd8298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 08:05:19,023 [    INFO] Logs saved in /home/jovyan/.local/state/dandi-cli/log/2025.04.05-08.05.18Z-1289.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH                 SIZE DONE    DONE% CHECKSUM STATUS    MESSAGE    \n",
      "001250/dandiset.yaml                             skipped   no change  \n",
      "Summary:                  0 Bytes                1 skipped 1 no change\n",
      "                          <0.00%                                      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'demo_sorting': Directory not empty\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download a local dandiset folder with yaml config\n",
    "os.system(\"dandi download --download dandiset.yaml DANDI:001250\")\n",
    "\n",
    "# write demo dataset to the local dandiset folder\n",
    "demo_path = \"./001250/spont.nwb\"\n",
    "\n",
    "# parametrize session file\n",
    "nwbfile = NWBFile(\n",
    "    session_description=\"Biophysical simulation of neuropixels in the spontaneous regime. Background noise and gain are fitted to Marques-Smith dataset.\",\n",
    "    identifier=str(uuid.uuid4()),\n",
    "    session_start_time=datetime.now(tzlocal()),\n",
    "    experimenter=\"Laquitaine Steeve\",\n",
    "    lab=\"Blue Brain Project\",\n",
    "    institution=\"EPFL\",\n",
    "    experiment_description=\"Biophysical simulation of neuropixels in the spontaneous regime. Background noise and gain are fitted to Marques-Smith dataset.\",\n",
    "    session_id=\"demo-npx-bio-spont\",\n",
    "    related_publications=\"https://doi.org/10.1101/2024.12.04.626805\",\n",
    "    keywords=[\"Biophysical simulation\", \"dense extracellular recordings\", \"spike sorting\"]\n",
    ")\n",
    "\n",
    "# subject metadata\n",
    "nwbfile.subject = Subject(\n",
    "    subject_id=\"demo-npx-bio-spont\",\n",
    "    species=\"Rattus norvegicus\",\n",
    "    age=\"P14D\",\n",
    "    sex=\"M\",\n",
    "    description=\"Wistar Rat\",\n",
    ")\n",
    "\n",
    "# bundle Extractors into nwb\n",
    "add_recording_to_nwbfile(recording=SmallRecording, nwbfile=nwbfile)\n",
    "add_sorting_to_nwbfile(sorting=SmallSorting, nwbfile=nwbfile)\n",
    "\n",
    "# write nwb file locally\n",
    "with NWBHDF5IO(path=demo_path, mode=\"w\") as io:\n",
    "    io.write(nwbfile)\n",
    "\n",
    "# check file with nwbinspector <source_folder> --config dandi\n",
    "\n",
    "# delete heavy intermediate datasets\n",
    "os.system(\"rm -rf demo_recording -rf demo_sorting\")\n",
    "\n",
    "# upload to dandi archive\n",
    "os.system(\n",
    "    f\"\"\"\n",
    "    export DANDI_API_KEY='210e68743286d64e84743bd8980d5771ef82bf4d';\n",
    "    cd 001250;\n",
    "    dandi organize {\"/home/jovyan/steevelaquitaine/spikebias/notebooks/0_demo/001250/spont.nwb\"} -f dry;\n",
    "    dandi organize {\"/home/jovyan/steevelaquitaine/spikebias/notebooks/0_demo/001250/spont.nwb\"};\n",
    "    dandi upload\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# clean up\n",
    "os.system(\"rm -rf 001250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3332f89-9106-4c1b-90e7-417df26b6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandi",
   "language": "python",
   "name": "dandi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
