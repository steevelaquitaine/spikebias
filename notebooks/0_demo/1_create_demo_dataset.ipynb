{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d22b34-dd1f-4f52-9688-8a4b8616e06c",
   "metadata": {},
   "source": [
    "# Load small dandiset\n",
    "\n",
    "author: laquitainesteeve@gmail.com\n",
    "\n",
    "purpose: create small demo dataset (neuropixels evoked 20 KHz)\n",
    "\n",
    "# Setup \n",
    "\n",
    "Activate dandi virtual environment (envs/dandi.yml)\n",
    "\n",
    "```bash\n",
    "python -m ipykernel install --user --name dandi --display-name \"dandi\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b8f812-1185-4419-ab2d-5e9879786b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/steevelaquitaine/spikebias/envs/dandi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SortingAnalyzer' from 'spikeinterface' (/home/jovyan/steevelaquitaine/spikebias/envs/dandi/lib/python3.10/site-packages/spikeinterface/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:12\u001b[0m\n",
      "File \u001b[0;32m~/steevelaquitaine/spikebias/envs/dandi/lib/python3.10/site-packages/neuroconv/tools/spikeinterface/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspikeinterface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     add_devices_to_nwbfile,\n\u001b[1;32m      3\u001b[0m     add_electrical_series_to_nwbfile,\n\u001b[1;32m      4\u001b[0m     add_electrode_groups_to_nwbfile,\n\u001b[1;32m      5\u001b[0m     add_electrodes_to_nwbfile,\n\u001b[1;32m      6\u001b[0m     add_recording_to_nwbfile,\n\u001b[1;32m      7\u001b[0m     add_sorting_to_nwbfile,\n\u001b[1;32m      8\u001b[0m     add_units_table_to_nwbfile,\n\u001b[1;32m      9\u001b[0m     add_sorting_analyzer_to_nwbfile,\n\u001b[1;32m     10\u001b[0m     write_recording_to_nwbfile,\n\u001b[1;32m     11\u001b[0m     write_sorting_to_nwbfile,\n\u001b[1;32m     12\u001b[0m     write_sorting_analyzer_to_nwbfile,\n\u001b[1;32m     13\u001b[0m     check_if_recording_traces_fit_into_memory,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspikeinterfacerecordingdatachunkiterator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_electrical_series_chunk_shape\n",
      "File \u001b[0;32m~/steevelaquitaine/spikebias/envs/dandi/lib/python3.10/site-packages/neuroconv/tools/spikeinterface/spikeinterface.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhdmf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AbstractDataChunkIterator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FilePath\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspikeinterface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseRecording, BaseSorting, SortingAnalyzer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspikeinterfacerecordingdatachunkiterator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     SpikeInterfaceRecordingDataChunkIterator,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnwb_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_module, make_or_load_nwbfile\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SortingAnalyzer' from 'spikeinterface' (/home/jovyan/steevelaquitaine/spikebias/envs/dandi/lib/python3.10/site-packages/spikeinterface/__init__.py)"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# import python packages\n",
    "import numpy as np\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface\n",
    "from pynwb.file import NWBFile, Subject\n",
    "from pynwb import NWBHDF5IO\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "print(\"spikeinterface\", spikeinterface.__version__)\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6693e-cd61-46e2-8369-56ae51cd7356",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94dd248-ff7f-444f-af11-3a6b5c9e2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_size(recording):\n",
    "    \"\"\"get size of RecordingExtractor in GB\n",
    "    \"\"\"\n",
    "    num_channels = recording.get_num_channels()\n",
    "    num_frames = recording.get_num_frames()\n",
    "    dtype_size = np.dtype(recording.get_dtype()).itemsize\n",
    "    size_bytes = num_channels * num_frames * dtype_size\n",
    "    size_gb = size_bytes / (1024**3)  # Convert bytes to GB\n",
    "    return size_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4059b-ad1f-4583-b619-64a5016ac027",
   "metadata": {},
   "source": [
    "## Load dandiset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b76324a-355f-4c91-9b27-3d9f89b9ae5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_path: https://dandiarchive.s3.amazonaws.com/blobs/9d6/6ed/9d66ed40-af31-43aa-b4ba-246d2206dcad\n",
      "\n",
      " NwbRecordingExtractor: 384 channels - 20.0kHz - 1 segments - 72,359,964 samples \n",
      "                       3,618.00s (1.00 hours) - float32 dtype - 103.51 GiB\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/9d6/6ed/9d66ed40-af31-43aa-b4ba-246d2206dcad\n",
      "\n",
      " NwbSortingExtractor: 1836 units - 1 segments - 20.0kHz\n",
      "  file_path: https://dandiarchive.s3.amazonaws.com/blobs/9d6/6ed/9d66ed40-af31-43aa-b4ba-246d2206dcad\n",
      "CPU times: user 174 ms, sys: 8.7 ms, total: 183 ms\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load dandiset (npx, evoked, 20Khz)\n",
    "dandiset_id = '001250'\n",
    "filepath = 'sub-002-fitted/sub-002-fitted_ecephys.nwb'\n",
    "\n",
    "# get the file path on S3\n",
    "with DandiAPIClient() as client:\n",
    "    asset = client.get_dandiset(dandiset_id, 'draft').get_asset_by_path(filepath)\n",
    "    s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "print(\"s3_path:\", s3_path)\n",
    "\n",
    "# get RecordingExtractor\n",
    "Recording = se.NwbRecordingExtractor(file_path=s3_path, stream_mode=\"remfile\")\n",
    "Sorting = se.NwbSortingExtractor(file_path=s3_path, stream_mode=\"remfile\")\n",
    "\n",
    "# report\n",
    "print('\\n', Recording)\n",
    "print('\\n', Sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803fbc08-fd36-4290-b828-a8ca46226367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 ms, sys: 7.91 ms, total: 27.5 ms\n",
      "Wall time: 176 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/steevelaquitaine/spikebias/envs/dandi/lib/python3.10/site-packages/spikeinterface/core/baserecordingsnippets.py:244: UserWarning: There is no Probe attached to this recording. Creating a dummy one with contact positions\n",
      "  warn(\"There is no Probe attached to this recording. Creating a dummy one with contact positions\")\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Re-wire probe because it is not always recognized by kilosort4\n",
    "probe = Recording.get_probe()\n",
    "Recording.set_probe(probe)\n",
    "\n",
    "# unit-test\n",
    "assert \"layers\" in Recording.get_property_keys(), \"RecordingExtractor should contain layer property\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f098c7-ee73-4688-adfa-8e7c2d1bfe66",
   "metadata": {},
   "source": [
    "## Make the small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7924863a-9fca-4641-92df-48feb1b0a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recording: ChannelSliceRecording: 120 channels - 20.0kHz - 1 segments - 72,359,964 samples \n",
      "                       3,618.00s (1.00 hours) - float32 dtype - 32.35 GiB\n",
      "\n",
      "Recording: FrameSliceRecording: 120 channels - 20.0kHz - 1 segments - 1,200,000 samples \n",
      "                     60.00s (1.00 minutes) - float32 dtype - 549.32 MiB\n",
      "\n",
      "Sorting: FrameSliceSorting: 1836 units - 1 segments - 20.0kHz\n"
     ]
    }
   ],
   "source": [
    "# select layer 5, 6 (most of the activity)\n",
    "selected_layers = ['L5', 'L6']\n",
    "channel_ids = Recording.channel_ids\n",
    "channel_ids = channel_ids[np.isin(Recording.get_property('layers'), selected_layers)]\n",
    "SmallRecording = Recording.channel_slice(channel_ids=channel_ids)\n",
    "print(\"\\nRecording:\", SmallRecording)\n",
    "\n",
    "# select first 2 minutes (~500 MB)\n",
    "sampling_rate = Recording.get_sampling_frequency() \n",
    "start_frame = 0\n",
    "end_frame = sampling_rate * 60\n",
    "SmallRecording = SmallRecording.frame_slice(start_frame=start_frame, end_frame=end_frame)\n",
    "SmallSorting = Sorting.frame_slice(start_frame=start_frame, end_frame=end_frame)\n",
    "\n",
    "print(\"\\nRecording:\", SmallRecording)\n",
    "print(\"\\nSorting:\", SmallSorting)\n",
    "\n",
    "# unit-test\n",
    "# - layers\n",
    "# - max spike times lower than number of frames\n",
    "assert (np.unique(SmallRecording.get_property('layers'))==selected_layers).all(), \"layers are not correct\"\n",
    "\n",
    "max_spike_time = max([SmallSorting.get_unit_spike_train(unit_id=unit).tolist() for unit in SmallSorting.get_unit_ids()])[0]\n",
    "assert max_spike_time < end_frame, \"max spike timestamp should be lower that the number of frames\"\n",
    "\n",
    "# Write [TODO]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandi",
   "language": "python",
   "name": "dandi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
