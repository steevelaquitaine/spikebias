# Setup python virtual environment for cebraspike pipeline
#
# author: steeve.laquitaine@epfl.ch
#
# Duration: 5 min
#
# TODO: list all dependencies and subdependencies with their versions
# to bypass the search for compatibility. Else this can take a while.
#
# 1. Install python3.9
#   
#   module purge
#   module load archive/2022-09 # load old archive (no need to use spack)
#   module load python-dev/0.4 # load python 3.9

# 2. Setup cebraspike virtual environment:
# 
#   mkdir /gpfs/bbp.cscs.ch/project/proj85/scratch/laquitai/preprint_2024/envs # create envs folder
#   python3.9 -m venv /gpfs/bbp.cscs.ch/project/proj85/scratch/laquitai/preprint_2024/envs/cebraspike3 # create env
#   source /gpfs/bbp.cscs.ch/project/proj85/scratch/laquitai/preprint_2024/envs/cebraspike3/bin/activate
#   python3.9 -m pip install -r /gpfs/bbp.cscs.ch/project/proj85/home/laquitai/spikebias/envs/cebraspike.txt
#
# 3. install cebra at the end

pynwb==2.8.2                # read nwb datasets
ipykernel==6.29.5           # setup notebook  
ipywidgets==8.0.6           # version important; or notebook and bluepy fail
jupyter==1.0.0              # version important: or notebook and and bluepy fail
pandas==1.3.4               # version important; bluepy fails with 1.5.3, works with 1.5.2
pyyaml==6.0.2               # setup config
scikit-learn==1.5.2         # machine learning
networkx==3.2.1             # graph machine learning
spikeinterface==0.100.5     # analysis and spike sorting of extracellular recordings
matplotlib==3.9.2           # plotting
ipython-autotime==0.3.2     # time track colab notebook cells
cebra==0.4.0
seaborn==0.13.2             # plotting
numba==0.60.0               # parallel computing
#--pre                       
#cebra[datasets,demos]      # dimensionality reduction
statsmodels==0.14.2         # baseline models